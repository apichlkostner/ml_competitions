{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forest cover type with multilayer perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "MAX_EPOCHS = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test_data(add_soil_features):\n",
    "    df_train = pd.read_csv(\"../input/train.csv\").sample(frac=1)\n",
    "    df_test = pd.read_csv(\"../input/test.csv\")\n",
    "\n",
    "    if add_soil_features:\n",
    "        rubbly = ['Soil_Type' + str(num) for num in [3, 4, 5, 10, 11, 13]]\n",
    "        stony  = ['Soil_Type' + str(num) for num in [6, 12]]\n",
    "        very_stony = ['Soil_Type' + str(num) for num in [9, 18, 26, 2]]\n",
    "        extemely_stony =['Soil_Type' + str(num) for num in [1, 24, 25, 27, 28, 29, 30, 31, 32, 33, 34, 36]]\n",
    "        rock_land_complex = ['Soil_Type' + str(num) for num in [11, 12, 34, 40]]\n",
    "        typic_cryaquolls_complex = ['Soil_Type' + str(num) for num in [20, 23]]\n",
    "        rock_outcrop_complex = ['Soil_Type' + str(num) for num in [1, 3, 4, 5, 6, 10, 27, 28 , 33]]\n",
    "        leighcan_family_complex = ['Soil_Type' + str(num) for num in [32, 39]]\n",
    "        leighcan = ['Soil_Type' + str(num) for num in [22, 23, 24, 25, 27, 28, 31]]\n",
    "        moran = ['Soil_Type' + str(num) for num in [39, 40]]\n",
    "\n",
    "        categories = [rubbly, stony, very_stony, extemely_stony, rock_land_complex, typic_cryaquolls_complex,\n",
    "                      rock_outcrop_complex, leighcan_family_complex, leighcan, moran]\n",
    "\n",
    "        cat_num = 0\n",
    "        for cat in categories:\n",
    "            cat_str = 'cat' + str(cat_num)\n",
    "            df_train[cat_str] = 0\n",
    "            df_test[cat_str] = 0\n",
    "            for soil_type in cat:\n",
    "                df_train[cat_str] += df_train[soil_type]\n",
    "                df_test[cat_str] += df_test[soil_type]\n",
    "            cat_num += 1\n",
    "            \n",
    "    return df_train, df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "from keras import regularizers\n",
    "\n",
    "def create_mode__sequential(num_features, num_categories):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(1024, activation='elu', input_dim=num_features, kernel_regularizer=regularizers.l2(0.000002)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(768, activation='elu', kernel_regularizer=regularizers.l2(0.000005)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(512, activation='elu', kernel_regularizer=regularizers.l2(0.000007)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(256, activation='elu', kernel_regularizer=regularizers.l2(0.000009)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(128, activation='elu', kernel_regularizer=regularizers.l2(0.00001)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(32, activation='elu', kernel_regularizer=regularizers.l2(0.00007)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(num_categories, activation='softmax'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import regularizers\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Activation, Dropout, BatchNormalization, Input\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers.merge import concatenate\n",
    "\n",
    "def create_model(num_features, num_categories):\n",
    "    inputs = Input(shape=(num_features,))\n",
    "    x = Dense(1024, activation='elu')(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    layer_1 = x\n",
    "\n",
    "    x = Dense(512, activation='elu')(layer_1)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    layer_2 = x\n",
    "\n",
    "    input_layer_3 = concatenate([layer_1, layer_2])\n",
    "    x = Dense(256, activation='elu')(input_layer_3)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    layer_3 = x\n",
    "\n",
    "    input_layer_4 = concatenate([layer_1, layer_2, layer_3])\n",
    "    x = Dense(64, activation='elu')(input_layer_4)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    layer_4 = x\n",
    "\n",
    "    input_layer_5 = concatenate([layer_2, layer_3, layer_4])\n",
    "    x = Dense(32, activation='elu')(input_layer_5)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "\n",
    "    outputs = Dense(num_categories, activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "def prepare_data_model(df_train, df_test):\n",
    "    X_train = np.array(df_train.drop(['Cover_Type', 'Id'], axis=1))\n",
    "    # original y is in range 1..7 -> convert to 0..6\n",
    "    y_train = np.array(df_train['Cover_Type']) - 1\n",
    "\n",
    "    scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "    X_scaled = scaler.transform(X_train)\n",
    "\n",
    "    num_categories = len(df_train['Cover_Type'].unique())\n",
    "    y_k = y_train.reshape([len(y_train), 1])\n",
    "    one_hot_labels = keras.utils.to_categorical(y_k)\n",
    "\n",
    "    model = create_model(X_train.shape[1], num_categories)\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'], optimizer='adam')\n",
    "    \n",
    "    return model, X_scaled, one_hot_labels, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_folders():\n",
    "    import os\n",
    "    \n",
    "    os.makedirs('history', exist_ok=True)\n",
    "    os.makedirs('checkpoints', exist_ok=True)\n",
    "    \n",
    "create_folders()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model without extra soil features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/lib/python3/dist-packages/keras/backend/tensorflow_backend.py:2880: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /usr/lib/python3/dist-packages/keras/backend/tensorflow_backend.py:1344: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "Train on 12096 samples, validate on 3024 samples\n",
      "Epoch 1/500\n",
      "12096/12096 [==============================] - 3s 243us/step - loss: 1.2339 - acc: 0.5389 - val_loss: 0.8100 - val_acc: 0.6673\n",
      "Epoch 2/500\n",
      "12096/12096 [==============================] - 2s 131us/step - loss: 0.9821 - acc: 0.6036 - val_loss: 0.7268 - val_acc: 0.7037\n",
      "Epoch 3/500\n",
      "12096/12096 [==============================] - 2s 132us/step - loss: 0.9034 - acc: 0.6279 - val_loss: 0.6874 - val_acc: 0.7186\n",
      "Epoch 4/500\n",
      "12096/12096 [==============================] - 2s 129us/step - loss: 0.8637 - acc: 0.6458 - val_loss: 0.6693 - val_acc: 0.7189\n",
      "Epoch 5/500\n",
      "12096/12096 [==============================] - 2s 129us/step - loss: 0.8226 - acc: 0.6648 - val_loss: 0.6557 - val_acc: 0.7252\n",
      "Epoch 6/500\n",
      "12096/12096 [==============================] - 2s 126us/step - loss: 0.8012 - acc: 0.6741 - val_loss: 0.6374 - val_acc: 0.7308\n",
      "Epoch 7/500\n",
      "12096/12096 [==============================] - 2s 125us/step - loss: 0.7943 - acc: 0.6810 - val_loss: 0.6306 - val_acc: 0.7302\n",
      "Epoch 8/500\n",
      "12096/12096 [==============================] - 2s 129us/step - loss: 0.7748 - acc: 0.6856 - val_loss: 0.6032 - val_acc: 0.7513\n",
      "Epoch 9/500\n",
      "12096/12096 [==============================] - 2s 125us/step - loss: 0.7710 - acc: 0.6795 - val_loss: 0.6090 - val_acc: 0.7470\n",
      "Epoch 10/500\n",
      "12096/12096 [==============================] - 2s 131us/step - loss: 0.7571 - acc: 0.6907 - val_loss: 0.6001 - val_acc: 0.7490\n",
      "Epoch 11/500\n",
      "12096/12096 [==============================] - 1s 122us/step - loss: 0.7486 - acc: 0.6963 - val_loss: 0.5983 - val_acc: 0.7503\n",
      "Epoch 12/500\n",
      "12096/12096 [==============================] - 2s 126us/step - loss: 0.7331 - acc: 0.7040 - val_loss: 0.5874 - val_acc: 0.7533\n",
      "Epoch 13/500\n",
      "12096/12096 [==============================] - 2s 126us/step - loss: 0.7197 - acc: 0.7088 - val_loss: 0.5749 - val_acc: 0.7596\n",
      "Epoch 14/500\n",
      "12096/12096 [==============================] - 2s 126us/step - loss: 0.7182 - acc: 0.7088 - val_loss: 0.5730 - val_acc: 0.7646\n",
      "Epoch 15/500\n",
      "12096/12096 [==============================] - 1s 122us/step - loss: 0.7118 - acc: 0.7164 - val_loss: 0.5612 - val_acc: 0.7646\n",
      "Epoch 16/500\n",
      "12096/12096 [==============================] - 2s 125us/step - loss: 0.7109 - acc: 0.7121 - val_loss: 0.5587 - val_acc: 0.7738\n",
      "Epoch 17/500\n",
      "12096/12096 [==============================] - 1s 121us/step - loss: 0.7034 - acc: 0.7145 - val_loss: 0.5595 - val_acc: 0.7622\n",
      "Epoch 18/500\n",
      "12096/12096 [==============================] - 1s 121us/step - loss: 0.6974 - acc: 0.7243 - val_loss: 0.5541 - val_acc: 0.7692\n",
      "Epoch 19/500\n",
      "12096/12096 [==============================] - 2s 125us/step - loss: 0.6824 - acc: 0.7258 - val_loss: 0.5474 - val_acc: 0.7761\n",
      "Epoch 20/500\n",
      "12096/12096 [==============================] - 1s 121us/step - loss: 0.6899 - acc: 0.7235 - val_loss: 0.5598 - val_acc: 0.7688\n",
      "Epoch 21/500\n",
      "12096/12096 [==============================] - 2s 128us/step - loss: 0.6769 - acc: 0.7299 - val_loss: 0.5360 - val_acc: 0.7821\n",
      "Epoch 22/500\n",
      "12096/12096 [==============================] - 1s 121us/step - loss: 0.6690 - acc: 0.7243 - val_loss: 0.5330 - val_acc: 0.7821\n",
      "Epoch 23/500\n",
      "12096/12096 [==============================] - 2s 126us/step - loss: 0.6584 - acc: 0.7322 - val_loss: 0.5293 - val_acc: 0.7854\n",
      "Epoch 24/500\n",
      "12096/12096 [==============================] - 1s 123us/step - loss: 0.6598 - acc: 0.7367 - val_loss: 0.5269 - val_acc: 0.7851\n",
      "Epoch 25/500\n",
      "12096/12096 [==============================] - 2s 126us/step - loss: 0.6626 - acc: 0.7332 - val_loss: 0.5237 - val_acc: 0.7854\n",
      "Epoch 26/500\n",
      "12096/12096 [==============================] - 2s 125us/step - loss: 0.6441 - acc: 0.7407 - val_loss: 0.5254 - val_acc: 0.7890\n",
      "Epoch 27/500\n",
      "12096/12096 [==============================] - 1s 117us/step - loss: 0.6514 - acc: 0.7371 - val_loss: 0.5114 - val_acc: 0.7877\n",
      "Epoch 28/500\n",
      "12096/12096 [==============================] - 1s 122us/step - loss: 0.6442 - acc: 0.7425 - val_loss: 0.5086 - val_acc: 0.7943\n",
      "Epoch 29/500\n",
      "12096/12096 [==============================] - 1s 118us/step - loss: 0.6465 - acc: 0.7403 - val_loss: 0.5060 - val_acc: 0.7900\n",
      "Epoch 30/500\n",
      "12096/12096 [==============================] - 2s 126us/step - loss: 0.6447 - acc: 0.7400 - val_loss: 0.5060 - val_acc: 0.7943\n",
      "Epoch 31/500\n",
      "12096/12096 [==============================] - 1s 124us/step - loss: 0.6299 - acc: 0.7487 - val_loss: 0.4908 - val_acc: 0.8052\n",
      "Epoch 32/500\n",
      "12096/12096 [==============================] - 1s 118us/step - loss: 0.6208 - acc: 0.7515 - val_loss: 0.4893 - val_acc: 0.7993\n",
      "Epoch 33/500\n",
      "12096/12096 [==============================] - 2s 147us/step - loss: 0.6232 - acc: 0.7503 - val_loss: 0.4929 - val_acc: 0.8065\n",
      "Epoch 34/500\n",
      "12096/12096 [==============================] - 1s 122us/step - loss: 0.6167 - acc: 0.7529 - val_loss: 0.4917 - val_acc: 0.7993\n",
      "Epoch 35/500\n",
      "12096/12096 [==============================] - 1s 122us/step - loss: 0.6133 - acc: 0.7561 - val_loss: 0.4833 - val_acc: 0.7999\n",
      "Epoch 36/500\n",
      "12096/12096 [==============================] - 1s 122us/step - loss: 0.6162 - acc: 0.7541 - val_loss: 0.4928 - val_acc: 0.7989\n",
      "Epoch 37/500\n",
      "12096/12096 [==============================] - 2s 129us/step - loss: 0.6194 - acc: 0.7524 - val_loss: 0.4812 - val_acc: 0.8062\n",
      "Epoch 38/500\n",
      "12096/12096 [==============================] - 1s 123us/step - loss: 0.6060 - acc: 0.7583 - val_loss: 0.4801 - val_acc: 0.8069\n",
      "Epoch 39/500\n",
      "12096/12096 [==============================] - 1s 121us/step - loss: 0.5993 - acc: 0.7613 - val_loss: 0.4728 - val_acc: 0.8089\n",
      "Epoch 40/500\n",
      "12096/12096 [==============================] - 1s 116us/step - loss: 0.6058 - acc: 0.7593 - val_loss: 0.4760 - val_acc: 0.8082\n",
      "Epoch 41/500\n",
      "12096/12096 [==============================] - 1s 120us/step - loss: 0.6002 - acc: 0.7631 - val_loss: 0.4714 - val_acc: 0.8165\n",
      "Epoch 42/500\n",
      "12096/12096 [==============================] - 1s 117us/step - loss: 0.5951 - acc: 0.7598 - val_loss: 0.4693 - val_acc: 0.8138\n",
      "Epoch 43/500\n",
      "12096/12096 [==============================] - 1s 121us/step - loss: 0.6036 - acc: 0.7599 - val_loss: 0.4623 - val_acc: 0.8214\n",
      "Epoch 44/500\n",
      "12096/12096 [==============================] - 1s 117us/step - loss: 0.5858 - acc: 0.7641 - val_loss: 0.4699 - val_acc: 0.8161\n",
      "Epoch 45/500\n",
      "12096/12096 [==============================] - 1s 116us/step - loss: 0.5962 - acc: 0.7609 - val_loss: 0.4651 - val_acc: 0.8188\n",
      "Epoch 46/500\n",
      "12096/12096 [==============================] - 2s 125us/step - loss: 0.5800 - acc: 0.7699 - val_loss: 0.4591 - val_acc: 0.8185\n",
      "Epoch 47/500\n",
      "12096/12096 [==============================] - 1s 115us/step - loss: 0.5798 - acc: 0.7725 - val_loss: 0.4616 - val_acc: 0.8171\n",
      "Epoch 48/500\n",
      "12096/12096 [==============================] - 1s 119us/step - loss: 0.5741 - acc: 0.7755 - val_loss: 0.4608 - val_acc: 0.8171\n",
      "Epoch 49/500\n",
      "12096/12096 [==============================] - 2s 126us/step - loss: 0.5678 - acc: 0.7767 - val_loss: 0.4608 - val_acc: 0.8132\n",
      "Epoch 50/500\n",
      "12096/12096 [==============================] - 2s 142us/step - loss: 0.5690 - acc: 0.7789 - val_loss: 0.4509 - val_acc: 0.8214\n",
      "Epoch 51/500\n",
      "12096/12096 [==============================] - 2s 131us/step - loss: 0.5628 - acc: 0.7791 - val_loss: 0.4506 - val_acc: 0.8221\n",
      "Epoch 52/500\n",
      "12096/12096 [==============================] - 1s 119us/step - loss: 0.5713 - acc: 0.7772 - val_loss: 0.4559 - val_acc: 0.8185\n",
      "Epoch 53/500\n",
      "12096/12096 [==============================] - 1s 118us/step - loss: 0.5643 - acc: 0.7774 - val_loss: 0.4514 - val_acc: 0.8175\n",
      "Epoch 54/500\n",
      "12096/12096 [==============================] - 1s 123us/step - loss: 0.5634 - acc: 0.7769 - val_loss: 0.4481 - val_acc: 0.8241\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/500\n",
      "12096/12096 [==============================] - 1s 120us/step - loss: 0.5572 - acc: 0.7792 - val_loss: 0.4506 - val_acc: 0.8165\n",
      "Epoch 56/500\n",
      "12096/12096 [==============================] - 1s 123us/step - loss: 0.5524 - acc: 0.7808 - val_loss: 0.4461 - val_acc: 0.8274\n",
      "Epoch 57/500\n",
      "12096/12096 [==============================] - 1s 118us/step - loss: 0.5489 - acc: 0.7827 - val_loss: 0.4390 - val_acc: 0.8267\n",
      "Epoch 58/500\n",
      "12096/12096 [==============================] - 1s 117us/step - loss: 0.5456 - acc: 0.7882 - val_loss: 0.4374 - val_acc: 0.8261\n",
      "Epoch 59/500\n",
      "12096/12096 [==============================] - 1s 121us/step - loss: 0.5484 - acc: 0.7829 - val_loss: 0.4414 - val_acc: 0.8323\n",
      "Epoch 60/500\n",
      "12096/12096 [==============================] - 1s 116us/step - loss: 0.5481 - acc: 0.7865 - val_loss: 0.4394 - val_acc: 0.8304\n",
      "Epoch 61/500\n",
      "12096/12096 [==============================] - 1s 116us/step - loss: 0.5407 - acc: 0.7875 - val_loss: 0.4340 - val_acc: 0.8254\n",
      "Epoch 62/500\n",
      "12096/12096 [==============================] - 1s 118us/step - loss: 0.5514 - acc: 0.7842 - val_loss: 0.4369 - val_acc: 0.8294\n",
      "Epoch 63/500\n",
      "12096/12096 [==============================] - 1s 119us/step - loss: 0.5444 - acc: 0.7853 - val_loss: 0.4299 - val_acc: 0.8264\n",
      "Epoch 64/500\n",
      "12096/12096 [==============================] - 1s 121us/step - loss: 0.5486 - acc: 0.7841 - val_loss: 0.4329 - val_acc: 0.8330\n",
      "Epoch 65/500\n",
      "12096/12096 [==============================] - 1s 117us/step - loss: 0.5312 - acc: 0.7911 - val_loss: 0.4253 - val_acc: 0.8310\n",
      "Epoch 66/500\n",
      "12096/12096 [==============================] - 1s 118us/step - loss: 0.5316 - acc: 0.7949 - val_loss: 0.4264 - val_acc: 0.8307\n",
      "Epoch 67/500\n",
      "12096/12096 [==============================] - 1s 117us/step - loss: 0.5355 - acc: 0.7884 - val_loss: 0.4351 - val_acc: 0.8231\n",
      "Epoch 68/500\n",
      "12096/12096 [==============================] - 1s 123us/step - loss: 0.5267 - acc: 0.7930 - val_loss: 0.4256 - val_acc: 0.8313\n",
      "Epoch 69/500\n",
      "12096/12096 [==============================] - 1s 123us/step - loss: 0.5243 - acc: 0.7898 - val_loss: 0.4299 - val_acc: 0.8333\n",
      "Epoch 70/500\n",
      "12096/12096 [==============================] - 1s 116us/step - loss: 0.5288 - acc: 0.7913 - val_loss: 0.4224 - val_acc: 0.8327\n",
      "Epoch 71/500\n",
      "12096/12096 [==============================] - 1s 120us/step - loss: 0.5299 - acc: 0.7881 - val_loss: 0.4264 - val_acc: 0.8337\n",
      "Epoch 72/500\n",
      "12096/12096 [==============================] - 1s 117us/step - loss: 0.5167 - acc: 0.7974 - val_loss: 0.4271 - val_acc: 0.8317\n",
      "Epoch 73/500\n",
      "12096/12096 [==============================] - 1s 121us/step - loss: 0.5237 - acc: 0.7975 - val_loss: 0.4215 - val_acc: 0.8360\n",
      "Epoch 74/500\n",
      "12096/12096 [==============================] - 1s 122us/step - loss: 0.5187 - acc: 0.7928 - val_loss: 0.4211 - val_acc: 0.8373\n",
      "Epoch 75/500\n",
      "12096/12096 [==============================] - 2s 125us/step - loss: 0.5148 - acc: 0.7988 - val_loss: 0.4273 - val_acc: 0.8373\n",
      "Epoch 76/500\n",
      "12096/12096 [==============================] - 1s 119us/step - loss: 0.5174 - acc: 0.7987 - val_loss: 0.4153 - val_acc: 0.8356\n",
      "Epoch 77/500\n",
      "12096/12096 [==============================] - 1s 119us/step - loss: 0.5056 - acc: 0.7993 - val_loss: 0.4191 - val_acc: 0.8390\n",
      "Epoch 78/500\n",
      "12096/12096 [==============================] - 1s 117us/step - loss: 0.5111 - acc: 0.7997 - val_loss: 0.4181 - val_acc: 0.8360\n",
      "Epoch 79/500\n",
      "12096/12096 [==============================] - 1s 121us/step - loss: 0.5105 - acc: 0.8017 - val_loss: 0.4198 - val_acc: 0.8413\n",
      "Epoch 80/500\n",
      "12096/12096 [==============================] - 1s 121us/step - loss: 0.5068 - acc: 0.8019 - val_loss: 0.4086 - val_acc: 0.8429\n",
      "Epoch 81/500\n",
      "12096/12096 [==============================] - 1s 121us/step - loss: 0.5022 - acc: 0.8039 - val_loss: 0.4096 - val_acc: 0.8439\n",
      "Epoch 82/500\n",
      "12096/12096 [==============================] - 1s 117us/step - loss: 0.4983 - acc: 0.8058 - val_loss: 0.4130 - val_acc: 0.8366\n",
      "Epoch 83/500\n",
      "12096/12096 [==============================] - 1s 117us/step - loss: 0.5095 - acc: 0.8020 - val_loss: 0.4247 - val_acc: 0.8337\n",
      "Epoch 84/500\n",
      "12096/12096 [==============================] - 1s 117us/step - loss: 0.5087 - acc: 0.7994 - val_loss: 0.4202 - val_acc: 0.8366\n",
      "Epoch 85/500\n",
      "12096/12096 [==============================] - 1s 121us/step - loss: 0.5045 - acc: 0.8037 - val_loss: 0.4104 - val_acc: 0.8482\n",
      "Epoch 86/500\n",
      "12096/12096 [==============================] - 1s 119us/step - loss: 0.4967 - acc: 0.8095 - val_loss: 0.4138 - val_acc: 0.8396\n",
      "Epoch 87/500\n",
      "12096/12096 [==============================] - 1s 116us/step - loss: 0.4964 - acc: 0.8031 - val_loss: 0.4061 - val_acc: 0.8472\n",
      "Epoch 88/500\n",
      "12096/12096 [==============================] - 1s 117us/step - loss: 0.5026 - acc: 0.8013 - val_loss: 0.4070 - val_acc: 0.8459\n",
      "Epoch 89/500\n",
      "12096/12096 [==============================] - 1s 122us/step - loss: 0.4969 - acc: 0.8058 - val_loss: 0.4053 - val_acc: 0.8485\n",
      "Epoch 90/500\n",
      "12096/12096 [==============================] - 1s 117us/step - loss: 0.4898 - acc: 0.8126 - val_loss: 0.4044 - val_acc: 0.8479\n",
      "Epoch 91/500\n",
      "12096/12096 [==============================] - 1s 118us/step - loss: 0.4908 - acc: 0.8095 - val_loss: 0.4103 - val_acc: 0.8419\n",
      "Epoch 92/500\n",
      "12096/12096 [==============================] - 1s 116us/step - loss: 0.4844 - acc: 0.8123 - val_loss: 0.4133 - val_acc: 0.8429\n",
      "Epoch 93/500\n",
      "12096/12096 [==============================] - 2s 129us/step - loss: 0.4823 - acc: 0.8098 - val_loss: 0.4089 - val_acc: 0.8452\n",
      "Epoch 94/500\n",
      "12096/12096 [==============================] - 1s 122us/step - loss: 0.4827 - acc: 0.8134 - val_loss: 0.4053 - val_acc: 0.8462\n",
      "Epoch 95/500\n",
      "12096/12096 [==============================] - 2s 130us/step - loss: 0.4818 - acc: 0.8154 - val_loss: 0.4108 - val_acc: 0.8413\n",
      "Epoch 96/500\n",
      "12096/12096 [==============================] - 2s 125us/step - loss: 0.4873 - acc: 0.8166 - val_loss: 0.4041 - val_acc: 0.8469\n",
      "Epoch 97/500\n",
      "12096/12096 [==============================] - 1s 117us/step - loss: 0.4907 - acc: 0.8099 - val_loss: 0.3995 - val_acc: 0.8469\n",
      "Epoch 98/500\n",
      "12096/12096 [==============================] - 1s 117us/step - loss: 0.4800 - acc: 0.8140 - val_loss: 0.4063 - val_acc: 0.8409\n",
      "Epoch 99/500\n",
      "12096/12096 [==============================] - 1s 124us/step - loss: 0.4739 - acc: 0.8219 - val_loss: 0.3945 - val_acc: 0.8505\n",
      "Epoch 100/500\n",
      "12096/12096 [==============================] - 2s 132us/step - loss: 0.4783 - acc: 0.8124 - val_loss: 0.4012 - val_acc: 0.8449\n",
      "Epoch 101/500\n",
      "12096/12096 [==============================] - 2s 129us/step - loss: 0.4707 - acc: 0.8197 - val_loss: 0.3943 - val_acc: 0.8555\n",
      "Epoch 102/500\n",
      "12096/12096 [==============================] - 1s 117us/step - loss: 0.4828 - acc: 0.8134 - val_loss: 0.3999 - val_acc: 0.8442\n",
      "Epoch 103/500\n",
      "12096/12096 [==============================] - 1s 118us/step - loss: 0.4687 - acc: 0.8214 - val_loss: 0.4006 - val_acc: 0.8489\n",
      "Epoch 104/500\n",
      "12096/12096 [==============================] - 2s 134us/step - loss: 0.4704 - acc: 0.8162 - val_loss: 0.3978 - val_acc: 0.8509\n",
      "Epoch 105/500\n",
      "12096/12096 [==============================] - 1s 123us/step - loss: 0.4699 - acc: 0.8173 - val_loss: 0.3992 - val_acc: 0.8515\n",
      "Epoch 106/500\n",
      "12096/12096 [==============================] - 1s 120us/step - loss: 0.4642 - acc: 0.8191 - val_loss: 0.3943 - val_acc: 0.8535\n",
      "Epoch 107/500\n",
      "12096/12096 [==============================] - 1s 120us/step - loss: 0.4715 - acc: 0.8132 - val_loss: 0.3940 - val_acc: 0.8512\n",
      "Epoch 108/500\n",
      "12096/12096 [==============================] - 1s 116us/step - loss: 0.4645 - acc: 0.8215 - val_loss: 0.3965 - val_acc: 0.8492\n",
      "Epoch 109/500\n",
      "12096/12096 [==============================] - 1s 124us/step - loss: 0.4625 - acc: 0.8235 - val_loss: 0.3930 - val_acc: 0.8585\n",
      "Epoch 110/500\n",
      "12096/12096 [==============================] - 1s 117us/step - loss: 0.4765 - acc: 0.8163 - val_loss: 0.3927 - val_acc: 0.8505\n",
      "Epoch 111/500\n",
      "12096/12096 [==============================] - 2s 129us/step - loss: 0.4690 - acc: 0.8159 - val_loss: 0.3908 - val_acc: 0.8522\n",
      "Epoch 112/500\n",
      "12096/12096 [==============================] - 2s 129us/step - loss: 0.4646 - acc: 0.8185 - val_loss: 0.3970 - val_acc: 0.8545\n",
      "Epoch 113/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12096/12096 [==============================] - 2s 135us/step - loss: 0.4633 - acc: 0.8230 - val_loss: 0.3907 - val_acc: 0.8515\n",
      "Epoch 114/500\n",
      "12096/12096 [==============================] - 2s 135us/step - loss: 0.4607 - acc: 0.8242 - val_loss: 0.3858 - val_acc: 0.8604\n",
      "Epoch 115/500\n",
      "12096/12096 [==============================] - 2s 130us/step - loss: 0.4608 - acc: 0.8202 - val_loss: 0.3882 - val_acc: 0.8522\n",
      "Epoch 116/500\n",
      "12096/12096 [==============================] - 2s 125us/step - loss: 0.4563 - acc: 0.8244 - val_loss: 0.3877 - val_acc: 0.8585\n",
      "Epoch 117/500\n",
      "12096/12096 [==============================] - 2s 125us/step - loss: 0.4673 - acc: 0.8200 - val_loss: 0.3932 - val_acc: 0.8505\n",
      "Epoch 118/500\n",
      "12096/12096 [==============================] - 1s 121us/step - loss: 0.4497 - acc: 0.8285 - val_loss: 0.3913 - val_acc: 0.8558\n",
      "Epoch 119/500\n",
      "12096/12096 [==============================] - 1s 117us/step - loss: 0.4517 - acc: 0.8273 - val_loss: 0.3896 - val_acc: 0.8555\n",
      "Epoch 120/500\n",
      "12096/12096 [==============================] - 1s 120us/step - loss: 0.4533 - acc: 0.8255 - val_loss: 0.3979 - val_acc: 0.8492\n",
      "Epoch 121/500\n",
      "12096/12096 [==============================] - 1s 118us/step - loss: 0.4469 - acc: 0.8284 - val_loss: 0.3905 - val_acc: 0.8495\n",
      "Epoch 122/500\n",
      "12096/12096 [==============================] - 1s 123us/step - loss: 0.4400 - acc: 0.8262 - val_loss: 0.3895 - val_acc: 0.8545\n",
      "Epoch 123/500\n",
      "12096/12096 [==============================] - 2s 124us/step - loss: 0.4530 - acc: 0.8279 - val_loss: 0.3867 - val_acc: 0.8578\n",
      "Epoch 124/500\n",
      "12096/12096 [==============================] - 2s 129us/step - loss: 0.4496 - acc: 0.8288 - val_loss: 0.3820 - val_acc: 0.8581\n",
      "Epoch 125/500\n",
      "12096/12096 [==============================] - 1s 124us/step - loss: 0.4478 - acc: 0.8267 - val_loss: 0.3862 - val_acc: 0.8538\n",
      "Epoch 126/500\n",
      "12096/12096 [==============================] - 2s 124us/step - loss: 0.4518 - acc: 0.8287 - val_loss: 0.3861 - val_acc: 0.8565\n",
      "Epoch 127/500\n",
      "12096/12096 [==============================] - 1s 116us/step - loss: 0.4442 - acc: 0.8299 - val_loss: 0.3832 - val_acc: 0.8568\n",
      "Epoch 128/500\n",
      "12096/12096 [==============================] - 1s 123us/step - loss: 0.4400 - acc: 0.8271 - val_loss: 0.3846 - val_acc: 0.8545\n",
      "Epoch 129/500\n",
      "12096/12096 [==============================] - 1s 116us/step - loss: 0.4393 - acc: 0.8327 - val_loss: 0.3808 - val_acc: 0.8522\n",
      "Epoch 130/500\n",
      "12096/12096 [==============================] - 1s 117us/step - loss: 0.4461 - acc: 0.8258 - val_loss: 0.3819 - val_acc: 0.8532\n",
      "Epoch 131/500\n",
      "12096/12096 [==============================] - 1s 117us/step - loss: 0.4443 - acc: 0.8299 - val_loss: 0.3837 - val_acc: 0.8591\n",
      "Epoch 132/500\n",
      "12096/12096 [==============================] - 1s 116us/step - loss: 0.4433 - acc: 0.8295 - val_loss: 0.3816 - val_acc: 0.8535\n",
      "Epoch 133/500\n",
      "12096/12096 [==============================] - 1s 117us/step - loss: 0.4451 - acc: 0.8314 - val_loss: 0.3790 - val_acc: 0.8568\n",
      "Epoch 134/500\n",
      "12096/12096 [==============================] - 1s 120us/step - loss: 0.4417 - acc: 0.8296 - val_loss: 0.3826 - val_acc: 0.8578\n",
      "Epoch 135/500\n",
      "12096/12096 [==============================] - 2s 126us/step - loss: 0.4389 - acc: 0.8335 - val_loss: 0.3880 - val_acc: 0.8575\n",
      "Epoch 136/500\n",
      "12096/12096 [==============================] - 1s 117us/step - loss: 0.4386 - acc: 0.8346 - val_loss: 0.3851 - val_acc: 0.8512\n",
      "Epoch 137/500\n",
      "12096/12096 [==============================] - 1s 123us/step - loss: 0.4405 - acc: 0.8350 - val_loss: 0.3817 - val_acc: 0.8585\n",
      "Epoch 138/500\n",
      "12096/12096 [==============================] - 1s 117us/step - loss: 0.4285 - acc: 0.8326 - val_loss: 0.3848 - val_acc: 0.8555\n",
      "Epoch 139/500\n",
      "12096/12096 [==============================] - 2s 125us/step - loss: 0.4314 - acc: 0.8337 - val_loss: 0.3892 - val_acc: 0.8552\n",
      "Epoch 140/500\n",
      "12096/12096 [==============================] - 1s 124us/step - loss: 0.4250 - acc: 0.8364 - val_loss: 0.3825 - val_acc: 0.8555\n",
      "Epoch 141/500\n",
      "12096/12096 [==============================] - 1s 121us/step - loss: 0.4374 - acc: 0.8309 - val_loss: 0.3802 - val_acc: 0.8575\n",
      "Epoch 142/500\n",
      "12096/12096 [==============================] - 1s 116us/step - loss: 0.4311 - acc: 0.8314 - val_loss: 0.3805 - val_acc: 0.8568\n",
      "Epoch 143/500\n",
      "12096/12096 [==============================] - 1s 115us/step - loss: 0.4347 - acc: 0.8314 - val_loss: 0.3815 - val_acc: 0.8548\n",
      "Epoch 144/500\n",
      "12096/12096 [==============================] - 1s 116us/step - loss: 0.4285 - acc: 0.8385 - val_loss: 0.3829 - val_acc: 0.8581\n",
      "Epoch 145/500\n",
      "12096/12096 [==============================] - 1s 118us/step - loss: 0.4260 - acc: 0.8395 - val_loss: 0.3850 - val_acc: 0.8548\n",
      "Epoch 146/500\n",
      "12096/12096 [==============================] - 1s 118us/step - loss: 0.4213 - acc: 0.8375 - val_loss: 0.3902 - val_acc: 0.8575\n",
      "Epoch 147/500\n",
      "12096/12096 [==============================] - 1s 117us/step - loss: 0.4250 - acc: 0.8372 - val_loss: 0.3729 - val_acc: 0.8601\n",
      "Epoch 148/500\n",
      "12096/12096 [==============================] - 1s 121us/step - loss: 0.4205 - acc: 0.8375 - val_loss: 0.3760 - val_acc: 0.8621\n",
      "Epoch 149/500\n",
      "12096/12096 [==============================] - 2s 129us/step - loss: 0.4198 - acc: 0.8379 - val_loss: 0.3808 - val_acc: 0.8562\n",
      "Epoch 150/500\n",
      "12096/12096 [==============================] - 2s 126us/step - loss: 0.4223 - acc: 0.8395 - val_loss: 0.3769 - val_acc: 0.8604\n",
      "Epoch 151/500\n",
      "12096/12096 [==============================] - 1s 120us/step - loss: 0.4168 - acc: 0.8382 - val_loss: 0.3832 - val_acc: 0.8558\n",
      "Epoch 152/500\n",
      "12096/12096 [==============================] - 2s 129us/step - loss: 0.4329 - acc: 0.8333 - val_loss: 0.3816 - val_acc: 0.8595\n",
      "Epoch 153/500\n",
      "12096/12096 [==============================] - 1s 117us/step - loss: 0.4206 - acc: 0.8394 - val_loss: 0.3790 - val_acc: 0.8575\n",
      "Epoch 154/500\n",
      "12096/12096 [==============================] - 1s 117us/step - loss: 0.4227 - acc: 0.8380 - val_loss: 0.3809 - val_acc: 0.8575\n",
      "Epoch 155/500\n",
      "12096/12096 [==============================] - 1s 117us/step - loss: 0.4162 - acc: 0.8433 - val_loss: 0.3797 - val_acc: 0.8555\n",
      "Epoch 156/500\n",
      "12096/12096 [==============================] - 1s 115us/step - loss: 0.4151 - acc: 0.8426 - val_loss: 0.3758 - val_acc: 0.8578\n",
      "Epoch 157/500\n",
      "12096/12096 [==============================] - 2s 130us/step - loss: 0.4230 - acc: 0.8446 - val_loss: 0.3719 - val_acc: 0.8562\n",
      "Epoch 158/500\n",
      "12096/12096 [==============================] - 1s 121us/step - loss: 0.4264 - acc: 0.8321 - val_loss: 0.3736 - val_acc: 0.8555\n",
      "Epoch 159/500\n",
      "12096/12096 [==============================] - 2s 124us/step - loss: 0.4167 - acc: 0.8414 - val_loss: 0.3704 - val_acc: 0.8591\n",
      "Epoch 160/500\n",
      "12096/12096 [==============================] - 1s 117us/step - loss: 0.4219 - acc: 0.8386 - val_loss: 0.3719 - val_acc: 0.8591\n",
      "Epoch 161/500\n",
      "12096/12096 [==============================] - 1s 116us/step - loss: 0.4226 - acc: 0.8393 - val_loss: 0.3740 - val_acc: 0.8614\n",
      "Epoch 162/500\n",
      "12096/12096 [==============================] - 1s 115us/step - loss: 0.4110 - acc: 0.8418 - val_loss: 0.3815 - val_acc: 0.8598\n",
      "Epoch 163/500\n",
      "12096/12096 [==============================] - 1s 117us/step - loss: 0.4193 - acc: 0.8396 - val_loss: 0.3683 - val_acc: 0.8608\n",
      "Epoch 164/500\n",
      "12096/12096 [==============================] - 1s 117us/step - loss: 0.4086 - acc: 0.8414 - val_loss: 0.3760 - val_acc: 0.8601\n",
      "Epoch 165/500\n",
      "12096/12096 [==============================] - 1s 117us/step - loss: 0.4031 - acc: 0.8433 - val_loss: 0.3783 - val_acc: 0.8598\n",
      "Epoch 166/500\n",
      "12096/12096 [==============================] - 1s 117us/step - loss: 0.4200 - acc: 0.8360 - val_loss: 0.3820 - val_acc: 0.8595\n",
      "Epoch 167/500\n",
      "12096/12096 [==============================] - 1s 117us/step - loss: 0.4122 - acc: 0.8418 - val_loss: 0.3811 - val_acc: 0.8588\n",
      "Epoch 168/500\n",
      "12096/12096 [==============================] - 1s 117us/step - loss: 0.4060 - acc: 0.8466 - val_loss: 0.3727 - val_acc: 0.8601\n",
      "Epoch 169/500\n",
      "12096/12096 [==============================] - 1s 117us/step - loss: 0.4160 - acc: 0.8389 - val_loss: 0.3717 - val_acc: 0.8601\n",
      "Epoch 170/500\n",
      "12096/12096 [==============================] - 1s 122us/step - loss: 0.4072 - acc: 0.8440 - val_loss: 0.3706 - val_acc: 0.8644\n",
      "Epoch 171/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12096/12096 [==============================] - 1s 116us/step - loss: 0.4157 - acc: 0.8374 - val_loss: 0.3738 - val_acc: 0.8598\n",
      "Epoch 172/500\n",
      "12096/12096 [==============================] - 1s 116us/step - loss: 0.4133 - acc: 0.8411 - val_loss: 0.3769 - val_acc: 0.8618\n",
      "Epoch 173/500\n",
      "12096/12096 [==============================] - 1s 116us/step - loss: 0.4037 - acc: 0.8471 - val_loss: 0.3733 - val_acc: 0.8614\n",
      "Epoch 174/500\n",
      "12096/12096 [==============================] - 1s 118us/step - loss: 0.4068 - acc: 0.8449 - val_loss: 0.3763 - val_acc: 0.8621\n",
      "Epoch 175/500\n",
      "12096/12096 [==============================] - 1s 120us/step - loss: 0.4060 - acc: 0.8447 - val_loss: 0.3728 - val_acc: 0.8621\n",
      "Epoch 176/500\n",
      "12096/12096 [==============================] - 2s 128us/step - loss: 0.4031 - acc: 0.8445 - val_loss: 0.3719 - val_acc: 0.8624\n",
      "Epoch 177/500\n",
      "12096/12096 [==============================] - 1s 117us/step - loss: 0.4048 - acc: 0.8452 - val_loss: 0.3725 - val_acc: 0.8621\n",
      "Epoch 178/500\n",
      "12096/12096 [==============================] - 1s 124us/step - loss: 0.3995 - acc: 0.8439 - val_loss: 0.3731 - val_acc: 0.8638\n",
      "Epoch 179/500\n",
      "12096/12096 [==============================] - 1s 122us/step - loss: 0.3986 - acc: 0.8500 - val_loss: 0.3692 - val_acc: 0.8657\n",
      "Epoch 180/500\n",
      "12096/12096 [==============================] - 1s 121us/step - loss: 0.3995 - acc: 0.8502 - val_loss: 0.3733 - val_acc: 0.8614\n",
      "Epoch 181/500\n",
      "12096/12096 [==============================] - 2s 125us/step - loss: 0.3973 - acc: 0.8497 - val_loss: 0.3744 - val_acc: 0.8641\n",
      "Epoch 182/500\n",
      "12096/12096 [==============================] - 1s 116us/step - loss: 0.3987 - acc: 0.8509 - val_loss: 0.3788 - val_acc: 0.8571\n",
      "Epoch 183/500\n",
      "12096/12096 [==============================] - 1s 123us/step - loss: 0.3979 - acc: 0.8486 - val_loss: 0.3756 - val_acc: 0.8621\n",
      "Epoch 184/500\n",
      "12096/12096 [==============================] - 1s 122us/step - loss: 0.4003 - acc: 0.8450 - val_loss: 0.3683 - val_acc: 0.8681\n",
      "Epoch 185/500\n",
      "12096/12096 [==============================] - 1s 116us/step - loss: 0.4010 - acc: 0.8486 - val_loss: 0.3701 - val_acc: 0.8624\n",
      "Epoch 186/500\n",
      "12096/12096 [==============================] - 2s 124us/step - loss: 0.4009 - acc: 0.8479 - val_loss: 0.3752 - val_acc: 0.8608\n",
      "Epoch 187/500\n",
      "12096/12096 [==============================] - 2s 128us/step - loss: 0.3953 - acc: 0.8495 - val_loss: 0.3746 - val_acc: 0.8631\n",
      "Epoch 188/500\n",
      "12096/12096 [==============================] - 1s 124us/step - loss: 0.3928 - acc: 0.8485 - val_loss: 0.3758 - val_acc: 0.8618\n",
      "Epoch 189/500\n",
      "12096/12096 [==============================] - 2s 126us/step - loss: 0.3951 - acc: 0.8496 - val_loss: 0.3715 - val_acc: 0.8654\n",
      "Epoch 190/500\n",
      "12096/12096 [==============================] - 2s 127us/step - loss: 0.3987 - acc: 0.8436 - val_loss: 0.3758 - val_acc: 0.8634\n",
      "Epoch 191/500\n",
      "12096/12096 [==============================] - 2s 126us/step - loss: 0.3964 - acc: 0.8459 - val_loss: 0.3735 - val_acc: 0.8634\n",
      "Epoch 192/500\n",
      "12096/12096 [==============================] - 1s 117us/step - loss: 0.3934 - acc: 0.8489 - val_loss: 0.3764 - val_acc: 0.8611\n",
      "Epoch 193/500\n",
      "12096/12096 [==============================] - 2s 127us/step - loss: 0.3912 - acc: 0.8480 - val_loss: 0.3739 - val_acc: 0.8624\n",
      "Epoch 194/500\n",
      "12096/12096 [==============================] - 1s 117us/step - loss: 0.3897 - acc: 0.8542 - val_loss: 0.3747 - val_acc: 0.8604\n",
      "Epoch 195/500\n",
      "12096/12096 [==============================] - 1s 117us/step - loss: 0.4010 - acc: 0.8461 - val_loss: 0.3713 - val_acc: 0.8608\n",
      "Epoch 196/500\n",
      "12096/12096 [==============================] - 1s 118us/step - loss: 0.4069 - acc: 0.8452 - val_loss: 0.3668 - val_acc: 0.8628\n",
      "Epoch 197/500\n",
      "12096/12096 [==============================] - 1s 122us/step - loss: 0.3942 - acc: 0.8489 - val_loss: 0.3705 - val_acc: 0.8595\n",
      "Epoch 198/500\n",
      "12096/12096 [==============================] - 1s 121us/step - loss: 0.3898 - acc: 0.8497 - val_loss: 0.3742 - val_acc: 0.8604\n",
      "Epoch 199/500\n",
      "12096/12096 [==============================] - 2s 127us/step - loss: 0.3866 - acc: 0.8547 - val_loss: 0.3668 - val_acc: 0.8664\n",
      "Epoch 200/500\n",
      "12096/12096 [==============================] - 1s 116us/step - loss: 0.3820 - acc: 0.8557 - val_loss: 0.3722 - val_acc: 0.8641\n",
      "Epoch 201/500\n",
      "12096/12096 [==============================] - 1s 116us/step - loss: 0.3954 - acc: 0.8495 - val_loss: 0.3660 - val_acc: 0.8641\n",
      "Epoch 202/500\n",
      "12096/12096 [==============================] - 1s 116us/step - loss: 0.3853 - acc: 0.8549 - val_loss: 0.3699 - val_acc: 0.8677\n",
      "Epoch 203/500\n",
      "12096/12096 [==============================] - 1s 116us/step - loss: 0.3923 - acc: 0.8465 - val_loss: 0.3700 - val_acc: 0.8638\n",
      "Epoch 204/500\n",
      "12096/12096 [==============================] - 1s 124us/step - loss: 0.3790 - acc: 0.8560 - val_loss: 0.3670 - val_acc: 0.8644\n",
      "Epoch 205/500\n",
      "12096/12096 [==============================] - 1s 118us/step - loss: 0.3825 - acc: 0.8542 - val_loss: 0.3705 - val_acc: 0.8634\n",
      "Epoch 206/500\n",
      "12096/12096 [==============================] - 1s 117us/step - loss: 0.3901 - acc: 0.8531 - val_loss: 0.3715 - val_acc: 0.8661\n",
      "Epoch 207/500\n",
      "12096/12096 [==============================] - 1s 117us/step - loss: 0.3800 - acc: 0.8541 - val_loss: 0.3683 - val_acc: 0.8657\n",
      "Epoch 208/500\n",
      "12096/12096 [==============================] - 1s 117us/step - loss: 0.3813 - acc: 0.8490 - val_loss: 0.3718 - val_acc: 0.8657\n",
      "Epoch 209/500\n",
      "12096/12096 [==============================] - 1s 120us/step - loss: 0.3761 - acc: 0.8594 - val_loss: 0.3680 - val_acc: 0.8654\n",
      "Epoch 210/500\n",
      "12096/12096 [==============================] - 1s 122us/step - loss: 0.3847 - acc: 0.8522 - val_loss: 0.3668 - val_acc: 0.8661\n",
      "Epoch 211/500\n",
      "12096/12096 [==============================] - 2s 127us/step - loss: 0.3842 - acc: 0.8534 - val_loss: 0.3586 - val_acc: 0.8694\n",
      "Epoch 212/500\n",
      "12096/12096 [==============================] - 1s 118us/step - loss: 0.3847 - acc: 0.8544 - val_loss: 0.3666 - val_acc: 0.8710\n",
      "Epoch 213/500\n",
      "12096/12096 [==============================] - 1s 118us/step - loss: 0.3811 - acc: 0.8568 - val_loss: 0.3673 - val_acc: 0.8657\n",
      "Epoch 214/500\n",
      "12096/12096 [==============================] - 1s 118us/step - loss: 0.3874 - acc: 0.8550 - val_loss: 0.3720 - val_acc: 0.8651\n",
      "Epoch 215/500\n",
      "12096/12096 [==============================] - 1s 117us/step - loss: 0.3882 - acc: 0.8535 - val_loss: 0.3687 - val_acc: 0.8667\n",
      "Epoch 216/500\n",
      "12096/12096 [==============================] - 1s 116us/step - loss: 0.3722 - acc: 0.8604 - val_loss: 0.3697 - val_acc: 0.8608\n",
      "Epoch 217/500\n",
      "12096/12096 [==============================] - 1s 116us/step - loss: 0.3915 - acc: 0.8506 - val_loss: 0.3714 - val_acc: 0.8628\n",
      "Epoch 218/500\n",
      "12096/12096 [==============================] - 2s 125us/step - loss: 0.3776 - acc: 0.8596 - val_loss: 0.3664 - val_acc: 0.8631\n",
      "Epoch 219/500\n",
      "12096/12096 [==============================] - 1s 117us/step - loss: 0.3832 - acc: 0.8557 - val_loss: 0.3654 - val_acc: 0.8651\n",
      "Epoch 220/500\n",
      "12096/12096 [==============================] - 1s 117us/step - loss: 0.3768 - acc: 0.8559 - val_loss: 0.3706 - val_acc: 0.8634\n",
      "Epoch 221/500\n",
      "12096/12096 [==============================] - 2s 129us/step - loss: 0.3697 - acc: 0.8609 - val_loss: 0.3742 - val_acc: 0.8647\n",
      "Epoch 222/500\n",
      "12096/12096 [==============================] - 1s 115us/step - loss: 0.3829 - acc: 0.8555 - val_loss: 0.3736 - val_acc: 0.8624\n",
      "Epoch 223/500\n",
      "12096/12096 [==============================] - 1s 116us/step - loss: 0.3841 - acc: 0.8530 - val_loss: 0.3713 - val_acc: 0.8694\n",
      "Epoch 224/500\n",
      "12096/12096 [==============================] - 1s 117us/step - loss: 0.3888 - acc: 0.8536 - val_loss: 0.3626 - val_acc: 0.8641\n",
      "Epoch 225/500\n",
      "12096/12096 [==============================] - 1s 115us/step - loss: 0.3676 - acc: 0.8555 - val_loss: 0.3659 - val_acc: 0.8667\n",
      "Epoch 226/500\n",
      "12096/12096 [==============================] - 1s 117us/step - loss: 0.3649 - acc: 0.8647 - val_loss: 0.3628 - val_acc: 0.8687\n",
      "Epoch 227/500\n",
      "12096/12096 [==============================] - 1s 116us/step - loss: 0.3766 - acc: 0.8573 - val_loss: 0.3646 - val_acc: 0.8677\n",
      "Epoch 228/500\n",
      "12096/12096 [==============================] - 1s 118us/step - loss: 0.3627 - acc: 0.8620 - val_loss: 0.3712 - val_acc: 0.8681\n",
      "Epoch 229/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12096/12096 [==============================] - 1s 115us/step - loss: 0.3693 - acc: 0.8604 - val_loss: 0.3647 - val_acc: 0.8667\n",
      "Epoch 230/500\n",
      "12096/12096 [==============================] - 1s 116us/step - loss: 0.3647 - acc: 0.8619 - val_loss: 0.3642 - val_acc: 0.8667\n",
      "Epoch 231/500\n",
      "12096/12096 [==============================] - 1s 115us/step - loss: 0.3794 - acc: 0.8569 - val_loss: 0.3642 - val_acc: 0.8634\n",
      "Epoch 232/500\n",
      "12096/12096 [==============================] - 1s 117us/step - loss: 0.3667 - acc: 0.8606 - val_loss: 0.3691 - val_acc: 0.8690\n",
      "Epoch 233/500\n",
      "12096/12096 [==============================] - 1s 123us/step - loss: 0.3740 - acc: 0.8612 - val_loss: 0.3666 - val_acc: 0.8697\n",
      "Epoch 234/500\n",
      "12096/12096 [==============================] - 2s 131us/step - loss: 0.3708 - acc: 0.8631 - val_loss: 0.3741 - val_acc: 0.8647\n",
      "Epoch 235/500\n",
      "12096/12096 [==============================] - 1s 117us/step - loss: 0.3717 - acc: 0.8576 - val_loss: 0.3671 - val_acc: 0.8710\n",
      "Epoch 236/500\n",
      "12096/12096 [==============================] - 1s 118us/step - loss: 0.3573 - acc: 0.8635 - val_loss: 0.3670 - val_acc: 0.8681\n",
      "Epoch 237/500\n",
      "12096/12096 [==============================] - 2s 126us/step - loss: 0.3811 - acc: 0.8566 - val_loss: 0.3673 - val_acc: 0.8631\n",
      "Epoch 238/500\n",
      "12096/12096 [==============================] - 1s 118us/step - loss: 0.3679 - acc: 0.8616 - val_loss: 0.3680 - val_acc: 0.8654\n",
      "Epoch 239/500\n",
      "12096/12096 [==============================] - 1s 118us/step - loss: 0.3580 - acc: 0.8621 - val_loss: 0.3686 - val_acc: 0.8710\n",
      "Epoch 240/500\n",
      "12096/12096 [==============================] - 2s 124us/step - loss: 0.3803 - acc: 0.8595 - val_loss: 0.3724 - val_acc: 0.8644\n",
      "Epoch 241/500\n",
      "12096/12096 [==============================] - 1s 124us/step - loss: 0.3562 - acc: 0.8668 - val_loss: 0.3614 - val_acc: 0.8714\n",
      "Epoch 242/500\n",
      "12096/12096 [==============================] - 1s 118us/step - loss: 0.3603 - acc: 0.8614 - val_loss: 0.3673 - val_acc: 0.8677\n",
      "Epoch 243/500\n",
      "12096/12096 [==============================] - 1s 121us/step - loss: 0.3660 - acc: 0.8619 - val_loss: 0.3633 - val_acc: 0.8677\n",
      "Epoch 244/500\n",
      "12096/12096 [==============================] - 2s 125us/step - loss: 0.3654 - acc: 0.8642 - val_loss: 0.3719 - val_acc: 0.8674\n",
      "Epoch 245/500\n",
      "12096/12096 [==============================] - 2s 131us/step - loss: 0.3717 - acc: 0.8585 - val_loss: 0.3689 - val_acc: 0.8667\n",
      "Epoch 246/500\n",
      "12096/12096 [==============================] - 1s 118us/step - loss: 0.3680 - acc: 0.8595 - val_loss: 0.3681 - val_acc: 0.8681\n",
      "Epoch 247/500\n",
      "12096/12096 [==============================] - 1s 116us/step - loss: 0.3623 - acc: 0.8612 - val_loss: 0.3670 - val_acc: 0.8700\n",
      "Epoch 248/500\n",
      "12096/12096 [==============================] - 1s 116us/step - loss: 0.3654 - acc: 0.8618 - val_loss: 0.3661 - val_acc: 0.8687\n",
      "Epoch 249/500\n",
      "12096/12096 [==============================] - 1s 117us/step - loss: 0.3687 - acc: 0.8602 - val_loss: 0.3698 - val_acc: 0.8647\n",
      "Epoch 250/500\n",
      "12096/12096 [==============================] - 2s 127us/step - loss: 0.3703 - acc: 0.8608 - val_loss: 0.3677 - val_acc: 0.8684\n",
      "Epoch 251/500\n",
      "12096/12096 [==============================] - 1s 120us/step - loss: 0.3675 - acc: 0.8599 - val_loss: 0.3628 - val_acc: 0.8700\n",
      "Epoch 252/500\n",
      "12096/12096 [==============================] - 1s 117us/step - loss: 0.3539 - acc: 0.8676 - val_loss: 0.3630 - val_acc: 0.8710\n",
      "Epoch 253/500\n",
      "12096/12096 [==============================] - 1s 116us/step - loss: 0.3690 - acc: 0.8609 - val_loss: 0.3656 - val_acc: 0.8694\n",
      "Epoch 254/500\n",
      "12096/12096 [==============================] - 1s 118us/step - loss: 0.3636 - acc: 0.8632 - val_loss: 0.3659 - val_acc: 0.8704\n",
      "Epoch 255/500\n",
      "12096/12096 [==============================] - 1s 118us/step - loss: 0.3621 - acc: 0.8617 - val_loss: 0.3625 - val_acc: 0.8657\n",
      "Epoch 256/500\n",
      "12096/12096 [==============================] - 1s 122us/step - loss: 0.3692 - acc: 0.8584 - val_loss: 0.3640 - val_acc: 0.8677\n",
      "Epoch 257/500\n",
      "12096/12096 [==============================] - 1s 123us/step - loss: 0.3535 - acc: 0.8655 - val_loss: 0.3648 - val_acc: 0.8697\n",
      "Epoch 258/500\n",
      "12096/12096 [==============================] - 2s 126us/step - loss: 0.3641 - acc: 0.8616 - val_loss: 0.3685 - val_acc: 0.8644\n",
      "Epoch 259/500\n",
      "12096/12096 [==============================] - 2s 126us/step - loss: 0.3600 - acc: 0.8636 - val_loss: 0.3611 - val_acc: 0.8700\n",
      "Epoch 260/500\n",
      "12096/12096 [==============================] - 1s 121us/step - loss: 0.3628 - acc: 0.8633 - val_loss: 0.3687 - val_acc: 0.8657\n",
      "Epoch 261/500\n",
      "12096/12096 [==============================] - 1s 116us/step - loss: 0.3614 - acc: 0.8632 - val_loss: 0.3699 - val_acc: 0.8714\n",
      "Epoch 262/500\n",
      "12096/12096 [==============================] - 1s 121us/step - loss: 0.3548 - acc: 0.8674 - val_loss: 0.3703 - val_acc: 0.8684\n",
      "Epoch 263/500\n",
      "12096/12096 [==============================] - 1s 117us/step - loss: 0.3549 - acc: 0.8674 - val_loss: 0.3624 - val_acc: 0.8700\n",
      "Epoch 264/500\n",
      "12096/12096 [==============================] - 1s 118us/step - loss: 0.3519 - acc: 0.8683 - val_loss: 0.3647 - val_acc: 0.8697\n",
      "Epoch 265/500\n",
      "12096/12096 [==============================] - 1s 121us/step - loss: 0.3523 - acc: 0.8640 - val_loss: 0.3612 - val_acc: 0.8750\n",
      "Epoch 266/500\n",
      "12096/12096 [==============================] - 1s 116us/step - loss: 0.3509 - acc: 0.8658 - val_loss: 0.3638 - val_acc: 0.8700\n",
      "Epoch 267/500\n",
      "12096/12096 [==============================] - 1s 117us/step - loss: 0.3492 - acc: 0.8667 - val_loss: 0.3672 - val_acc: 0.8664\n",
      "Epoch 268/500\n",
      "12096/12096 [==============================] - 1s 118us/step - loss: 0.3532 - acc: 0.8639 - val_loss: 0.3649 - val_acc: 0.8737\n",
      "Epoch 269/500\n",
      "12096/12096 [==============================] - 1s 115us/step - loss: 0.3661 - acc: 0.8619 - val_loss: 0.3597 - val_acc: 0.8747\n",
      "Epoch 270/500\n",
      "12096/12096 [==============================] - 1s 116us/step - loss: 0.3592 - acc: 0.8669 - val_loss: 0.3654 - val_acc: 0.8704\n",
      "Epoch 271/500\n",
      "12096/12096 [==============================] - 1s 115us/step - loss: 0.3517 - acc: 0.8684 - val_loss: 0.3625 - val_acc: 0.8694\n",
      "Epoch 272/500\n",
      "12096/12096 [==============================] - 2s 124us/step - loss: 0.3514 - acc: 0.8670 - val_loss: 0.3625 - val_acc: 0.8714\n",
      "Epoch 273/500\n",
      "12096/12096 [==============================] - 2s 131us/step - loss: 0.3563 - acc: 0.8681 - val_loss: 0.3636 - val_acc: 0.8717\n",
      "Epoch 274/500\n",
      "12096/12096 [==============================] - 1s 119us/step - loss: 0.3441 - acc: 0.8714 - val_loss: 0.3565 - val_acc: 0.8737\n",
      "Epoch 275/500\n",
      "12096/12096 [==============================] - 1s 123us/step - loss: 0.3468 - acc: 0.8708 - val_loss: 0.3644 - val_acc: 0.8733\n",
      "Epoch 276/500\n",
      "12096/12096 [==============================] - 1s 118us/step - loss: 0.3519 - acc: 0.8675 - val_loss: 0.3641 - val_acc: 0.8720\n",
      "Epoch 277/500\n",
      "12096/12096 [==============================] - 1s 122us/step - loss: 0.3499 - acc: 0.8712 - val_loss: 0.3654 - val_acc: 0.8724\n",
      "Epoch 278/500\n",
      "12096/12096 [==============================] - 1s 117us/step - loss: 0.3461 - acc: 0.8685 - val_loss: 0.3694 - val_acc: 0.8700\n",
      "Epoch 279/500\n",
      "12096/12096 [==============================] - 1s 121us/step - loss: 0.3486 - acc: 0.8673 - val_loss: 0.3619 - val_acc: 0.8770\n",
      "Epoch 280/500\n",
      "12096/12096 [==============================] - 1s 117us/step - loss: 0.3455 - acc: 0.8682 - val_loss: 0.3596 - val_acc: 0.8720\n",
      "Epoch 281/500\n",
      "12096/12096 [==============================] - 2s 125us/step - loss: 0.3585 - acc: 0.8684 - val_loss: 0.3674 - val_acc: 0.8687\n",
      "Epoch 282/500\n",
      "12096/12096 [==============================] - 2s 126us/step - loss: 0.3437 - acc: 0.8685 - val_loss: 0.3663 - val_acc: 0.8724\n",
      "Epoch 283/500\n",
      "12096/12096 [==============================] - 2s 126us/step - loss: 0.3460 - acc: 0.8681 - val_loss: 0.3686 - val_acc: 0.8684\n",
      "Epoch 284/500\n",
      "12096/12096 [==============================] - 2s 126us/step - loss: 0.3545 - acc: 0.8705 - val_loss: 0.3641 - val_acc: 0.8677\n",
      "Epoch 285/500\n",
      "12096/12096 [==============================] - 2s 129us/step - loss: 0.3496 - acc: 0.8719 - val_loss: 0.3674 - val_acc: 0.8700\n",
      "Epoch 286/500\n",
      "12096/12096 [==============================] - 1s 123us/step - loss: 0.3341 - acc: 0.8722 - val_loss: 0.3660 - val_acc: 0.8733\n",
      "Epoch 287/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12096/12096 [==============================] - 1s 121us/step - loss: 0.3426 - acc: 0.8691 - val_loss: 0.3648 - val_acc: 0.8697\n",
      "Epoch 288/500\n",
      "12096/12096 [==============================] - 1s 122us/step - loss: 0.3421 - acc: 0.8724 - val_loss: 0.3700 - val_acc: 0.8690\n",
      "Epoch 289/500\n",
      "12096/12096 [==============================] - 1s 123us/step - loss: 0.3453 - acc: 0.8719 - val_loss: 0.3650 - val_acc: 0.8707\n",
      "Epoch 290/500\n",
      "12096/12096 [==============================] - 2s 130us/step - loss: 0.3475 - acc: 0.8696 - val_loss: 0.3611 - val_acc: 0.8700\n",
      "Epoch 291/500\n",
      "12096/12096 [==============================] - 1s 118us/step - loss: 0.3389 - acc: 0.8691 - val_loss: 0.3626 - val_acc: 0.8684\n",
      "Epoch 292/500\n",
      "12096/12096 [==============================] - 1s 120us/step - loss: 0.3484 - acc: 0.8672 - val_loss: 0.3654 - val_acc: 0.8664\n",
      "Epoch 293/500\n",
      "12096/12096 [==============================] - 1s 119us/step - loss: 0.3444 - acc: 0.8674 - val_loss: 0.3654 - val_acc: 0.8684\n",
      "Epoch 294/500\n",
      "12096/12096 [==============================] - 1s 116us/step - loss: 0.3452 - acc: 0.8698 - val_loss: 0.3717 - val_acc: 0.8700\n",
      "Epoch 295/500\n",
      "12096/12096 [==============================] - 1s 118us/step - loss: 0.3387 - acc: 0.8710 - val_loss: 0.3653 - val_acc: 0.8687\n",
      "Epoch 296/500\n",
      "12096/12096 [==============================] - 1s 118us/step - loss: 0.3480 - acc: 0.8701 - val_loss: 0.3636 - val_acc: 0.8737\n",
      "Epoch 297/500\n",
      "12096/12096 [==============================] - 1s 119us/step - loss: 0.3350 - acc: 0.8755 - val_loss: 0.3625 - val_acc: 0.8720\n",
      "Epoch 298/500\n",
      "12096/12096 [==============================] - 1s 119us/step - loss: 0.3394 - acc: 0.8701 - val_loss: 0.3637 - val_acc: 0.8730\n",
      "Epoch 299/500\n",
      "12096/12096 [==============================] - 1s 116us/step - loss: 0.3436 - acc: 0.8693 - val_loss: 0.3678 - val_acc: 0.8674\n",
      "Epoch 300/500\n",
      "12096/12096 [==============================] - 1s 117us/step - loss: 0.3351 - acc: 0.8734 - val_loss: 0.3667 - val_acc: 0.8687\n",
      "Epoch 301/500\n",
      "12096/12096 [==============================] - 1s 117us/step - loss: 0.3378 - acc: 0.8733 - val_loss: 0.3642 - val_acc: 0.8727\n",
      "Epoch 302/500\n",
      "12096/12096 [==============================] - 1s 118us/step - loss: 0.3378 - acc: 0.8707 - val_loss: 0.3644 - val_acc: 0.8733\n",
      "Epoch 303/500\n",
      "12096/12096 [==============================] - 1s 116us/step - loss: 0.3413 - acc: 0.8714 - val_loss: 0.3607 - val_acc: 0.8760\n",
      "Epoch 304/500\n",
      "12096/12096 [==============================] - 1s 117us/step - loss: 0.3446 - acc: 0.8726 - val_loss: 0.3602 - val_acc: 0.8743\n",
      "Epoch 305/500\n",
      "12096/12096 [==============================] - 1s 122us/step - loss: 0.3498 - acc: 0.8676 - val_loss: 0.3635 - val_acc: 0.8707\n",
      "Epoch 306/500\n",
      "12096/12096 [==============================] - 1s 120us/step - loss: 0.3327 - acc: 0.8727 - val_loss: 0.3618 - val_acc: 0.8697\n",
      "Epoch 307/500\n",
      "12096/12096 [==============================] - 1s 120us/step - loss: 0.3352 - acc: 0.8740 - val_loss: 0.3633 - val_acc: 0.8727\n",
      "Epoch 308/500\n",
      "12096/12096 [==============================] - 1s 120us/step - loss: 0.3413 - acc: 0.8717 - val_loss: 0.3603 - val_acc: 0.8687\n",
      "Epoch 309/500\n",
      "12096/12096 [==============================] - 1s 121us/step - loss: 0.3258 - acc: 0.8786 - val_loss: 0.3626 - val_acc: 0.8720\n",
      "Epoch 310/500\n",
      "12096/12096 [==============================] - 1s 116us/step - loss: 0.3388 - acc: 0.8736 - val_loss: 0.3636 - val_acc: 0.8697\n",
      "Epoch 311/500\n",
      "12096/12096 [==============================] - 1s 117us/step - loss: 0.3346 - acc: 0.8730 - val_loss: 0.3624 - val_acc: 0.8720\n",
      "Epoch 312/500\n",
      "12096/12096 [==============================] - 1s 118us/step - loss: 0.3294 - acc: 0.8769 - val_loss: 0.3680 - val_acc: 0.8717\n",
      "Epoch 313/500\n",
      "12096/12096 [==============================] - 1s 121us/step - loss: 0.3372 - acc: 0.8753 - val_loss: 0.3667 - val_acc: 0.8733\n",
      "Epoch 314/500\n",
      "12096/12096 [==============================] - 1s 117us/step - loss: 0.3318 - acc: 0.8776 - val_loss: 0.3662 - val_acc: 0.8704\n",
      "Epoch 315/500\n",
      "12096/12096 [==============================] - 1s 117us/step - loss: 0.3348 - acc: 0.8715 - val_loss: 0.3666 - val_acc: 0.8750\n",
      "Epoch 316/500\n",
      "12096/12096 [==============================] - 1s 116us/step - loss: 0.3235 - acc: 0.8774 - val_loss: 0.3648 - val_acc: 0.8750\n",
      "Epoch 317/500\n",
      "12096/12096 [==============================] - 1s 117us/step - loss: 0.3294 - acc: 0.8748 - val_loss: 0.3637 - val_acc: 0.8757\n",
      "Epoch 318/500\n",
      "12096/12096 [==============================] - 1s 116us/step - loss: 0.3277 - acc: 0.8796 - val_loss: 0.3605 - val_acc: 0.8750\n",
      "Epoch 319/500\n",
      "12096/12096 [==============================] - 1s 117us/step - loss: 0.3371 - acc: 0.8728 - val_loss: 0.3642 - val_acc: 0.8717\n",
      "Epoch 320/500\n",
      "12096/12096 [==============================] - 1s 116us/step - loss: 0.3269 - acc: 0.8753 - val_loss: 0.3665 - val_acc: 0.8724\n",
      "Epoch 321/500\n",
      "12096/12096 [==============================] - 1s 118us/step - loss: 0.3310 - acc: 0.8735 - val_loss: 0.3632 - val_acc: 0.8710\n",
      "Epoch 322/500\n",
      "12096/12096 [==============================] - 1s 117us/step - loss: 0.3325 - acc: 0.8759 - val_loss: 0.3682 - val_acc: 0.8710\n",
      "Epoch 323/500\n",
      "12096/12096 [==============================] - 1s 117us/step - loss: 0.3384 - acc: 0.8722 - val_loss: 0.3664 - val_acc: 0.8720\n",
      "Epoch 324/500\n",
      "12096/12096 [==============================] - 1s 118us/step - loss: 0.3337 - acc: 0.8763 - val_loss: 0.3667 - val_acc: 0.8757\n",
      "Epoch 325/500\n",
      "12096/12096 [==============================] - 1s 117us/step - loss: 0.3143 - acc: 0.8810 - val_loss: 0.3695 - val_acc: 0.8720\n",
      "Epoch 326/500\n",
      "12096/12096 [==============================] - 1s 118us/step - loss: 0.3314 - acc: 0.8760 - val_loss: 0.3693 - val_acc: 0.8687\n",
      "Epoch 327/500\n",
      "12096/12096 [==============================] - 1s 117us/step - loss: 0.3273 - acc: 0.8783 - val_loss: 0.3678 - val_acc: 0.8681\n",
      "Epoch 328/500\n",
      "12096/12096 [==============================] - 1s 118us/step - loss: 0.3239 - acc: 0.8793 - val_loss: 0.3636 - val_acc: 0.8714\n",
      "Epoch 329/500\n",
      "12096/12096 [==============================] - 1s 117us/step - loss: 0.3302 - acc: 0.8765 - val_loss: 0.3658 - val_acc: 0.8720\n",
      "Epoch 330/500\n",
      "12096/12096 [==============================] - 1s 117us/step - loss: 0.3276 - acc: 0.8772 - val_loss: 0.3598 - val_acc: 0.8763\n",
      "Epoch 331/500\n",
      "12096/12096 [==============================] - 1s 117us/step - loss: 0.3190 - acc: 0.8820 - val_loss: 0.3627 - val_acc: 0.8714\n",
      "Epoch 332/500\n",
      "12096/12096 [==============================] - 1s 118us/step - loss: 0.3307 - acc: 0.8755 - val_loss: 0.3676 - val_acc: 0.8690\n",
      "Epoch 333/500\n",
      "12096/12096 [==============================] - 1s 117us/step - loss: 0.3300 - acc: 0.8771 - val_loss: 0.3549 - val_acc: 0.8757\n",
      "Epoch 334/500\n",
      "12096/12096 [==============================] - 1s 116us/step - loss: 0.3404 - acc: 0.8725 - val_loss: 0.3580 - val_acc: 0.8767\n",
      "Epoch 335/500\n",
      "12096/12096 [==============================] - 1s 118us/step - loss: 0.3321 - acc: 0.8762 - val_loss: 0.3658 - val_acc: 0.8684\n",
      "Epoch 336/500\n",
      "12096/12096 [==============================] - 1s 117us/step - loss: 0.3256 - acc: 0.8805 - val_loss: 0.3634 - val_acc: 0.8704\n",
      "Epoch 337/500\n",
      "12096/12096 [==============================] - 1s 118us/step - loss: 0.3151 - acc: 0.8827 - val_loss: 0.3623 - val_acc: 0.8720\n",
      "Epoch 338/500\n",
      "12096/12096 [==============================] - 1s 118us/step - loss: 0.3270 - acc: 0.8767 - val_loss: 0.3597 - val_acc: 0.8710\n",
      "Epoch 339/500\n",
      "12096/12096 [==============================] - 1s 118us/step - loss: 0.3250 - acc: 0.8781 - val_loss: 0.3675 - val_acc: 0.8690\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f0ae72a2e80>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, CSVLogger\n",
    "\n",
    "name = 'mlp_01'\n",
    "\n",
    "df_train, df_test = get_train_test_data(add_soil_features=False)\n",
    "model, X_scaled, one_hot_labels, scaler = prepare_data_model(df_train, df_test)\n",
    "\n",
    "# log history\n",
    "history_name = 'history/' + name + '.csv'\n",
    "historyLogger = CSVLogger(history_name, append=False)\n",
    "\n",
    "# safe model checkpoints\n",
    "checkpoint_name = 'checkpoints/' + name + '.hdf5'\n",
    "modelCheckPoint = ModelCheckpoint(checkpoint_name,\n",
    "                                    monitor='val_acc', verbose=0, save_best_only=True)\n",
    "\n",
    "# stop training when validation loss is not decreasing\n",
    "earlyStopping = EarlyStopping(monitor='val_acc', min_delta=0.001, patience=60)\n",
    "\n",
    "# list with callbacks for fit_generator\n",
    "callbacks = [historyLogger, modelCheckPoint, earlyStopping]\n",
    "\n",
    "model.fit(X_scaled, one_hot_labels, epochs=MAX_EPOCHS, batch_size=64, validation_split=0.2, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0ae80a6f28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "history = pd.read_csv(history_name)\n",
    "\n",
    "fig, ax = plt.subplots(1,1)\n",
    "ax.set_title('Accuracy for train and test set')\n",
    "ax.set_xlabel('Epochs')\n",
    "ax.set_ylabel('Accuracy')\n",
    "\n",
    "x_plt = np.arange(0, len(history))\n",
    "l1 = ax.plot(x_plt, history['acc'], label='Train')\n",
    "l2 = ax.plot(x_plt, history['val_acc'], label='Test')\n",
    "\n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "X_test = np.array(df_test.drop(['Id'], axis=1))\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "model.load_weights(checkpoint_name)\n",
    "pred = model.predict(X_test_scaled)\n",
    "# convert from range 0..6 back to 1..7\n",
    "y_pred = np.argmax(pred, axis=1) + 1\n",
    "\n",
    "df_test = df_test.iloc[:, df_test.columns == 'Id']\n",
    "df_test['Cover_Type'] = y_pred\n",
    "\n",
    "df_test.to_csv('result_nn_1.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model using the extra soil features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12096 samples, validate on 3024 samples\n",
      "Epoch 1/500\n",
      "12096/12096 [==============================] - 3s 249us/step - loss: 1.2088 - acc: 0.5491 - val_loss: 0.8590 - val_acc: 0.6491\n",
      "Epoch 2/500\n",
      "12096/12096 [==============================] - 2s 133us/step - loss: 0.9666 - acc: 0.6059 - val_loss: 0.7658 - val_acc: 0.6786\n",
      "Epoch 3/500\n",
      "12096/12096 [==============================] - 1s 123us/step - loss: 0.8958 - acc: 0.6330 - val_loss: 0.7335 - val_acc: 0.6968\n",
      "Epoch 4/500\n",
      "12096/12096 [==============================] - 1s 122us/step - loss: 0.8607 - acc: 0.6521 - val_loss: 0.7041 - val_acc: 0.7027\n",
      "Epoch 5/500\n",
      "12096/12096 [==============================] - 1s 124us/step - loss: 0.8304 - acc: 0.6639 - val_loss: 0.6878 - val_acc: 0.7080\n",
      "Epoch 6/500\n",
      "12096/12096 [==============================] - 2s 127us/step - loss: 0.8077 - acc: 0.6667 - val_loss: 0.7037 - val_acc: 0.7077\n",
      "Epoch 7/500\n",
      "12096/12096 [==============================] - 2s 130us/step - loss: 0.8023 - acc: 0.6734 - val_loss: 0.6650 - val_acc: 0.7179\n",
      "Epoch 8/500\n",
      "12096/12096 [==============================] - 2s 130us/step - loss: 0.7748 - acc: 0.6864 - val_loss: 0.6523 - val_acc: 0.7269\n",
      "Epoch 9/500\n",
      "12096/12096 [==============================] - 2s 139us/step - loss: 0.7709 - acc: 0.6887 - val_loss: 0.6459 - val_acc: 0.7272\n",
      "Epoch 10/500\n",
      "12096/12096 [==============================] - 2s 129us/step - loss: 0.7665 - acc: 0.6879 - val_loss: 0.6404 - val_acc: 0.7288\n",
      "Epoch 11/500\n",
      "12096/12096 [==============================] - 2s 132us/step - loss: 0.7546 - acc: 0.6962 - val_loss: 0.6249 - val_acc: 0.7417\n",
      "Epoch 12/500\n",
      "12096/12096 [==============================] - 2s 131us/step - loss: 0.7383 - acc: 0.7000 - val_loss: 0.6256 - val_acc: 0.7417\n",
      "Epoch 13/500\n",
      "12096/12096 [==============================] - 1s 122us/step - loss: 0.7254 - acc: 0.7063 - val_loss: 0.6147 - val_acc: 0.7407\n",
      "Epoch 14/500\n",
      "12096/12096 [==============================] - 2s 127us/step - loss: 0.7184 - acc: 0.7127 - val_loss: 0.6076 - val_acc: 0.7513\n",
      "Epoch 15/500\n",
      "12096/12096 [==============================] - 2s 125us/step - loss: 0.7204 - acc: 0.7085 - val_loss: 0.6102 - val_acc: 0.7424\n",
      "Epoch 16/500\n",
      "12096/12096 [==============================] - 2s 124us/step - loss: 0.7054 - acc: 0.7146 - val_loss: 0.5960 - val_acc: 0.7503\n",
      "Epoch 17/500\n",
      "12096/12096 [==============================] - 1s 122us/step - loss: 0.6998 - acc: 0.7151 - val_loss: 0.6002 - val_acc: 0.7477\n",
      "Epoch 18/500\n",
      "12096/12096 [==============================] - 2s 127us/step - loss: 0.6915 - acc: 0.7150 - val_loss: 0.5788 - val_acc: 0.7622\n",
      "Epoch 19/500\n",
      "12096/12096 [==============================] - 2s 132us/step - loss: 0.6876 - acc: 0.7240 - val_loss: 0.5683 - val_acc: 0.7612\n",
      "Epoch 20/500\n",
      "12096/12096 [==============================] - 2s 137us/step - loss: 0.6863 - acc: 0.7237 - val_loss: 0.5698 - val_acc: 0.7606\n",
      "Epoch 21/500\n",
      "12096/12096 [==============================] - 2s 134us/step - loss: 0.6766 - acc: 0.7250 - val_loss: 0.5610 - val_acc: 0.7702\n",
      "Epoch 22/500\n",
      "12096/12096 [==============================] - 2s 130us/step - loss: 0.6769 - acc: 0.7291 - val_loss: 0.5569 - val_acc: 0.7646\n",
      "Epoch 23/500\n",
      "12096/12096 [==============================] - 2s 129us/step - loss: 0.6639 - acc: 0.7310 - val_loss: 0.5485 - val_acc: 0.7745\n",
      "Epoch 24/500\n",
      "12096/12096 [==============================] - 2s 133us/step - loss: 0.6646 - acc: 0.7296 - val_loss: 0.5494 - val_acc: 0.7755\n",
      "Epoch 25/500\n",
      "12096/12096 [==============================] - 2s 130us/step - loss: 0.6636 - acc: 0.7352 - val_loss: 0.5350 - val_acc: 0.7781\n",
      "Epoch 26/500\n",
      "12096/12096 [==============================] - 2s 134us/step - loss: 0.6474 - acc: 0.7411 - val_loss: 0.5412 - val_acc: 0.7784\n",
      "Epoch 27/500\n",
      "12096/12096 [==============================] - 2s 132us/step - loss: 0.6392 - acc: 0.7409 - val_loss: 0.5288 - val_acc: 0.7877\n",
      "Epoch 28/500\n",
      "12096/12096 [==============================] - 2s 129us/step - loss: 0.6405 - acc: 0.7397 - val_loss: 0.5392 - val_acc: 0.7765\n",
      "Epoch 29/500\n",
      "12096/12096 [==============================] - 2s 130us/step - loss: 0.6305 - acc: 0.7450 - val_loss: 0.5322 - val_acc: 0.7791\n",
      "Epoch 30/500\n",
      "12096/12096 [==============================] - 2s 129us/step - loss: 0.6383 - acc: 0.7399 - val_loss: 0.5279 - val_acc: 0.7827\n",
      "Epoch 31/500\n",
      "12096/12096 [==============================] - 2s 128us/step - loss: 0.6368 - acc: 0.7444 - val_loss: 0.5349 - val_acc: 0.7798\n",
      "Epoch 32/500\n",
      "12096/12096 [==============================] - 2s 130us/step - loss: 0.6261 - acc: 0.7468 - val_loss: 0.5345 - val_acc: 0.7874\n",
      "Epoch 33/500\n",
      "12096/12096 [==============================] - 2s 129us/step - loss: 0.6277 - acc: 0.7475 - val_loss: 0.5249 - val_acc: 0.7811\n",
      "Epoch 34/500\n",
      "12096/12096 [==============================] - 2s 125us/step - loss: 0.6204 - acc: 0.7480 - val_loss: 0.5234 - val_acc: 0.7837\n",
      "Epoch 35/500\n",
      "12096/12096 [==============================] - 1s 123us/step - loss: 0.6133 - acc: 0.7571 - val_loss: 0.5138 - val_acc: 0.7870\n",
      "Epoch 36/500\n",
      "12096/12096 [==============================] - 1s 123us/step - loss: 0.6167 - acc: 0.7535 - val_loss: 0.5246 - val_acc: 0.7814\n",
      "Epoch 37/500\n",
      "12096/12096 [==============================] - 2s 129us/step - loss: 0.6062 - acc: 0.7556 - val_loss: 0.5072 - val_acc: 0.7953\n",
      "Epoch 38/500\n",
      "12096/12096 [==============================] - 2s 124us/step - loss: 0.6015 - acc: 0.7618 - val_loss: 0.5053 - val_acc: 0.7907\n",
      "Epoch 39/500\n",
      "12096/12096 [==============================] - 2s 125us/step - loss: 0.6023 - acc: 0.7581 - val_loss: 0.5113 - val_acc: 0.7867\n",
      "Epoch 40/500\n",
      "12096/12096 [==============================] - 2s 131us/step - loss: 0.6011 - acc: 0.7651 - val_loss: 0.5002 - val_acc: 0.7956\n",
      "Epoch 41/500\n",
      "12096/12096 [==============================] - 2s 130us/step - loss: 0.5982 - acc: 0.7622 - val_loss: 0.5047 - val_acc: 0.7966\n",
      "Epoch 42/500\n",
      "12096/12096 [==============================] - 1s 122us/step - loss: 0.5972 - acc: 0.7630 - val_loss: 0.5056 - val_acc: 0.7930\n",
      "Epoch 43/500\n",
      "12096/12096 [==============================] - 2s 128us/step - loss: 0.5776 - acc: 0.7712 - val_loss: 0.4996 - val_acc: 0.7989\n",
      "Epoch 44/500\n",
      "12096/12096 [==============================] - 2s 129us/step - loss: 0.5860 - acc: 0.7671 - val_loss: 0.4860 - val_acc: 0.8029\n",
      "Epoch 45/500\n",
      "12096/12096 [==============================] - 2s 127us/step - loss: 0.5735 - acc: 0.7747 - val_loss: 0.4987 - val_acc: 0.7983\n",
      "Epoch 46/500\n",
      "12096/12096 [==============================] - 2s 126us/step - loss: 0.5791 - acc: 0.7691 - val_loss: 0.4887 - val_acc: 0.8039\n",
      "Epoch 47/500\n",
      "12096/12096 [==============================] - 2s 126us/step - loss: 0.5736 - acc: 0.7738 - val_loss: 0.5009 - val_acc: 0.7970\n",
      "Epoch 48/500\n",
      "12096/12096 [==============================] - 2s 126us/step - loss: 0.5805 - acc: 0.7748 - val_loss: 0.4817 - val_acc: 0.8036\n",
      "Epoch 49/500\n",
      "12096/12096 [==============================] - 2s 128us/step - loss: 0.5703 - acc: 0.7743 - val_loss: 0.4908 - val_acc: 0.8072\n",
      "Epoch 50/500\n",
      "12096/12096 [==============================] - 2s 126us/step - loss: 0.5673 - acc: 0.7772 - val_loss: 0.4863 - val_acc: 0.8006\n",
      "Epoch 51/500\n",
      "12096/12096 [==============================] - 2s 129us/step - loss: 0.5722 - acc: 0.7725 - val_loss: 0.4872 - val_acc: 0.8042\n",
      "Epoch 52/500\n",
      "12096/12096 [==============================] - 1s 123us/step - loss: 0.5686 - acc: 0.7714 - val_loss: 0.4772 - val_acc: 0.8009\n",
      "Epoch 53/500\n",
      "12096/12096 [==============================] - 2s 125us/step - loss: 0.5658 - acc: 0.7763 - val_loss: 0.4810 - val_acc: 0.8026\n",
      "Epoch 54/500\n",
      "12096/12096 [==============================] - 2s 129us/step - loss: 0.5543 - acc: 0.7810 - val_loss: 0.4779 - val_acc: 0.8105\n",
      "Epoch 55/500\n",
      "12096/12096 [==============================] - 1s 123us/step - loss: 0.5646 - acc: 0.7803 - val_loss: 0.4812 - val_acc: 0.8082\n",
      "Epoch 56/500\n",
      "12096/12096 [==============================] - 1s 123us/step - loss: 0.5558 - acc: 0.7817 - val_loss: 0.4824 - val_acc: 0.8082\n",
      "Epoch 57/500\n",
      "12096/12096 [==============================] - 2s 129us/step - loss: 0.5466 - acc: 0.7820 - val_loss: 0.4649 - val_acc: 0.8138\n",
      "Epoch 58/500\n",
      "12096/12096 [==============================] - 1s 124us/step - loss: 0.5491 - acc: 0.7841 - val_loss: 0.4678 - val_acc: 0.8132\n",
      "Epoch 59/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12096/12096 [==============================] - 2s 127us/step - loss: 0.5533 - acc: 0.7821 - val_loss: 0.4608 - val_acc: 0.8148\n",
      "Epoch 60/500\n",
      "12096/12096 [==============================] - 2s 126us/step - loss: 0.5409 - acc: 0.7869 - val_loss: 0.4665 - val_acc: 0.8185\n",
      "Epoch 61/500\n",
      "12096/12096 [==============================] - 1s 121us/step - loss: 0.5459 - acc: 0.7859 - val_loss: 0.4740 - val_acc: 0.8029\n",
      "Epoch 62/500\n",
      "12096/12096 [==============================] - 1s 121us/step - loss: 0.5377 - acc: 0.7882 - val_loss: 0.4670 - val_acc: 0.8085\n",
      "Epoch 63/500\n",
      "12096/12096 [==============================] - 1s 122us/step - loss: 0.5384 - acc: 0.7896 - val_loss: 0.4637 - val_acc: 0.8142\n",
      "Epoch 64/500\n",
      "12096/12096 [==============================] - 1s 122us/step - loss: 0.5346 - acc: 0.7909 - val_loss: 0.4630 - val_acc: 0.8171\n",
      "Epoch 65/500\n",
      "12096/12096 [==============================] - 1s 121us/step - loss: 0.5431 - acc: 0.7865 - val_loss: 0.4651 - val_acc: 0.8175\n",
      "Epoch 66/500\n",
      "12096/12096 [==============================] - 2s 125us/step - loss: 0.5316 - acc: 0.7916 - val_loss: 0.4607 - val_acc: 0.8204\n",
      "Epoch 67/500\n",
      "12096/12096 [==============================] - 1s 122us/step - loss: 0.5269 - acc: 0.7971 - val_loss: 0.4576 - val_acc: 0.8151\n",
      "Epoch 68/500\n",
      "12096/12096 [==============================] - 1s 124us/step - loss: 0.5261 - acc: 0.7921 - val_loss: 0.4596 - val_acc: 0.8148\n",
      "Epoch 69/500\n",
      "12096/12096 [==============================] - 1s 124us/step - loss: 0.5239 - acc: 0.7964 - val_loss: 0.4604 - val_acc: 0.8158\n",
      "Epoch 70/500\n",
      "12096/12096 [==============================] - 1s 121us/step - loss: 0.5251 - acc: 0.7926 - val_loss: 0.4579 - val_acc: 0.8181\n",
      "Epoch 71/500\n",
      "12096/12096 [==============================] - 2s 126us/step - loss: 0.5254 - acc: 0.7972 - val_loss: 0.4528 - val_acc: 0.8221\n",
      "Epoch 72/500\n",
      "12096/12096 [==============================] - 2s 127us/step - loss: 0.5145 - acc: 0.8005 - val_loss: 0.4449 - val_acc: 0.8237\n",
      "Epoch 73/500\n",
      "12096/12096 [==============================] - 2s 126us/step - loss: 0.5255 - acc: 0.7970 - val_loss: 0.4480 - val_acc: 0.8257\n",
      "Epoch 74/500\n",
      "12096/12096 [==============================] - 1s 123us/step - loss: 0.5168 - acc: 0.7922 - val_loss: 0.4456 - val_acc: 0.8231\n",
      "Epoch 75/500\n",
      "12096/12096 [==============================] - 2s 125us/step - loss: 0.5152 - acc: 0.8032 - val_loss: 0.4543 - val_acc: 0.8228\n",
      "Epoch 76/500\n",
      "12096/12096 [==============================] - 2s 125us/step - loss: 0.5100 - acc: 0.7959 - val_loss: 0.4423 - val_acc: 0.8214\n",
      "Epoch 77/500\n",
      "12096/12096 [==============================] - 1s 122us/step - loss: 0.5091 - acc: 0.8016 - val_loss: 0.4457 - val_acc: 0.8191\n",
      "Epoch 78/500\n",
      "12096/12096 [==============================] - 2s 127us/step - loss: 0.5090 - acc: 0.8000 - val_loss: 0.4373 - val_acc: 0.8284\n",
      "Epoch 79/500\n",
      "12096/12096 [==============================] - 1s 122us/step - loss: 0.5004 - acc: 0.8043 - val_loss: 0.4369 - val_acc: 0.8277\n",
      "Epoch 80/500\n",
      "12096/12096 [==============================] - 1s 123us/step - loss: 0.5004 - acc: 0.8037 - val_loss: 0.4410 - val_acc: 0.8264\n",
      "Epoch 81/500\n",
      "12096/12096 [==============================] - 1s 119us/step - loss: 0.5002 - acc: 0.8053 - val_loss: 0.4357 - val_acc: 0.8284\n",
      "Epoch 82/500\n",
      "12096/12096 [==============================] - 2s 127us/step - loss: 0.5016 - acc: 0.8020 - val_loss: 0.4336 - val_acc: 0.8300\n",
      "Epoch 83/500\n",
      "12096/12096 [==============================] - 1s 122us/step - loss: 0.4988 - acc: 0.8065 - val_loss: 0.4337 - val_acc: 0.8280\n",
      "Epoch 84/500\n",
      "12096/12096 [==============================] - 1s 122us/step - loss: 0.5056 - acc: 0.8021 - val_loss: 0.4399 - val_acc: 0.8274\n",
      "Epoch 85/500\n",
      "12096/12096 [==============================] - 1s 124us/step - loss: 0.5064 - acc: 0.8063 - val_loss: 0.4392 - val_acc: 0.8228\n",
      "Epoch 86/500\n",
      "12096/12096 [==============================] - 1s 122us/step - loss: 0.4970 - acc: 0.8053 - val_loss: 0.4389 - val_acc: 0.8267\n",
      "Epoch 87/500\n",
      "12096/12096 [==============================] - 1s 121us/step - loss: 0.4917 - acc: 0.8094 - val_loss: 0.4315 - val_acc: 0.8277\n",
      "Epoch 88/500\n",
      "12096/12096 [==============================] - 2s 124us/step - loss: 0.4917 - acc: 0.8092 - val_loss: 0.4284 - val_acc: 0.8317\n",
      "Epoch 89/500\n",
      "12096/12096 [==============================] - 1s 121us/step - loss: 0.4925 - acc: 0.8092 - val_loss: 0.4269 - val_acc: 0.8317\n",
      "Epoch 90/500\n",
      "12096/12096 [==============================] - 1s 122us/step - loss: 0.4899 - acc: 0.8103 - val_loss: 0.4449 - val_acc: 0.8280\n",
      "Epoch 91/500\n",
      "12096/12096 [==============================] - 2s 125us/step - loss: 0.4990 - acc: 0.8049 - val_loss: 0.4306 - val_acc: 0.8294\n",
      "Epoch 92/500\n",
      "12096/12096 [==============================] - 2s 127us/step - loss: 0.4903 - acc: 0.8142 - val_loss: 0.4314 - val_acc: 0.8340\n",
      "Epoch 93/500\n",
      "12096/12096 [==============================] - 2s 130us/step - loss: 0.4932 - acc: 0.8092 - val_loss: 0.4333 - val_acc: 0.8356\n",
      "Epoch 94/500\n",
      "12096/12096 [==============================] - 1s 122us/step - loss: 0.4863 - acc: 0.8118 - val_loss: 0.4277 - val_acc: 0.8330\n",
      "Epoch 95/500\n",
      "12096/12096 [==============================] - 1s 122us/step - loss: 0.4802 - acc: 0.8078 - val_loss: 0.4299 - val_acc: 0.8297\n",
      "Epoch 96/500\n",
      "12096/12096 [==============================] - 1s 123us/step - loss: 0.4774 - acc: 0.8115 - val_loss: 0.4312 - val_acc: 0.8307\n",
      "Epoch 97/500\n",
      "12096/12096 [==============================] - 1s 121us/step - loss: 0.4868 - acc: 0.8148 - val_loss: 0.4233 - val_acc: 0.8317\n",
      "Epoch 98/500\n",
      "12096/12096 [==============================] - 1s 123us/step - loss: 0.4912 - acc: 0.8106 - val_loss: 0.4302 - val_acc: 0.8307\n",
      "Epoch 99/500\n",
      "12096/12096 [==============================] - 1s 121us/step - loss: 0.4784 - acc: 0.8132 - val_loss: 0.4285 - val_acc: 0.8337\n",
      "Epoch 100/500\n",
      "12096/12096 [==============================] - 2s 127us/step - loss: 0.4622 - acc: 0.8179 - val_loss: 0.4236 - val_acc: 0.8406\n",
      "Epoch 101/500\n",
      "12096/12096 [==============================] - 1s 121us/step - loss: 0.4867 - acc: 0.8117 - val_loss: 0.4282 - val_acc: 0.8333\n",
      "Epoch 102/500\n",
      "12096/12096 [==============================] - 2s 127us/step - loss: 0.4796 - acc: 0.8137 - val_loss: 0.4207 - val_acc: 0.8413\n",
      "Epoch 103/500\n",
      "12096/12096 [==============================] - 1s 122us/step - loss: 0.4687 - acc: 0.8163 - val_loss: 0.4165 - val_acc: 0.8373\n",
      "Epoch 104/500\n",
      "12096/12096 [==============================] - 2s 126us/step - loss: 0.4768 - acc: 0.8159 - val_loss: 0.4145 - val_acc: 0.8419\n",
      "Epoch 105/500\n",
      "12096/12096 [==============================] - 2s 130us/step - loss: 0.4687 - acc: 0.8197 - val_loss: 0.4163 - val_acc: 0.8439\n",
      "Epoch 106/500\n",
      "12096/12096 [==============================] - 1s 122us/step - loss: 0.4738 - acc: 0.8175 - val_loss: 0.4100 - val_acc: 0.8419\n",
      "Epoch 107/500\n",
      "12096/12096 [==============================] - 1s 122us/step - loss: 0.4664 - acc: 0.8169 - val_loss: 0.4251 - val_acc: 0.8403\n",
      "Epoch 108/500\n",
      "12096/12096 [==============================] - 1s 121us/step - loss: 0.4616 - acc: 0.8256 - val_loss: 0.4153 - val_acc: 0.8433\n",
      "Epoch 109/500\n",
      "12096/12096 [==============================] - 1s 123us/step - loss: 0.4627 - acc: 0.8228 - val_loss: 0.4221 - val_acc: 0.8416\n",
      "Epoch 110/500\n",
      "12096/12096 [==============================] - 1s 120us/step - loss: 0.4736 - acc: 0.8204 - val_loss: 0.4123 - val_acc: 0.8403\n",
      "Epoch 111/500\n",
      "12096/12096 [==============================] - 1s 122us/step - loss: 0.4566 - acc: 0.8238 - val_loss: 0.4189 - val_acc: 0.8433\n",
      "Epoch 112/500\n",
      "12096/12096 [==============================] - 2s 128us/step - loss: 0.4578 - acc: 0.8234 - val_loss: 0.4088 - val_acc: 0.8482\n",
      "Epoch 113/500\n",
      "12096/12096 [==============================] - 2s 124us/step - loss: 0.4549 - acc: 0.8247 - val_loss: 0.4131 - val_acc: 0.8429\n",
      "Epoch 114/500\n",
      "12096/12096 [==============================] - 1s 123us/step - loss: 0.4614 - acc: 0.8225 - val_loss: 0.4070 - val_acc: 0.8390\n",
      "Epoch 115/500\n",
      "12096/12096 [==============================] - 1s 124us/step - loss: 0.4566 - acc: 0.8243 - val_loss: 0.4123 - val_acc: 0.8406\n",
      "Epoch 116/500\n",
      "12096/12096 [==============================] - 1s 123us/step - loss: 0.4611 - acc: 0.8248 - val_loss: 0.4100 - val_acc: 0.8456\n",
      "Epoch 117/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12096/12096 [==============================] - 1s 123us/step - loss: 0.4586 - acc: 0.8217 - val_loss: 0.4082 - val_acc: 0.8413\n",
      "Epoch 118/500\n",
      "12096/12096 [==============================] - 1s 123us/step - loss: 0.4676 - acc: 0.8189 - val_loss: 0.4177 - val_acc: 0.8406\n",
      "Epoch 119/500\n",
      "12096/12096 [==============================] - 1s 122us/step - loss: 0.4436 - acc: 0.8309 - val_loss: 0.4141 - val_acc: 0.8396\n",
      "Epoch 120/500\n",
      "12096/12096 [==============================] - 1s 120us/step - loss: 0.4453 - acc: 0.8274 - val_loss: 0.4181 - val_acc: 0.8446\n",
      "Epoch 121/500\n",
      "12096/12096 [==============================] - 2s 127us/step - loss: 0.4474 - acc: 0.8293 - val_loss: 0.4039 - val_acc: 0.8489\n",
      "Epoch 122/500\n",
      "12096/12096 [==============================] - 2s 125us/step - loss: 0.4525 - acc: 0.8251 - val_loss: 0.4066 - val_acc: 0.8492\n",
      "Epoch 123/500\n",
      "12096/12096 [==============================] - 1s 122us/step - loss: 0.4487 - acc: 0.8275 - val_loss: 0.4135 - val_acc: 0.8462\n",
      "Epoch 124/500\n",
      "12096/12096 [==============================] - 1s 122us/step - loss: 0.4523 - acc: 0.8280 - val_loss: 0.4119 - val_acc: 0.8433\n",
      "Epoch 125/500\n",
      "12096/12096 [==============================] - 1s 120us/step - loss: 0.4388 - acc: 0.8304 - val_loss: 0.4112 - val_acc: 0.8492\n",
      "Epoch 126/500\n",
      "12096/12096 [==============================] - 1s 121us/step - loss: 0.4410 - acc: 0.8290 - val_loss: 0.4149 - val_acc: 0.8433\n",
      "Epoch 127/500\n",
      "12096/12096 [==============================] - 2s 125us/step - loss: 0.4452 - acc: 0.8266 - val_loss: 0.4043 - val_acc: 0.8409\n",
      "Epoch 128/500\n",
      "12096/12096 [==============================] - 1s 122us/step - loss: 0.4424 - acc: 0.8295 - val_loss: 0.4118 - val_acc: 0.8482\n",
      "Epoch 129/500\n",
      "12096/12096 [==============================] - 1s 121us/step - loss: 0.4489 - acc: 0.8286 - val_loss: 0.4030 - val_acc: 0.8489\n",
      "Epoch 130/500\n",
      "12096/12096 [==============================] - 1s 122us/step - loss: 0.4381 - acc: 0.8302 - val_loss: 0.4019 - val_acc: 0.8446\n",
      "Epoch 131/500\n",
      "12096/12096 [==============================] - 1s 121us/step - loss: 0.4334 - acc: 0.8335 - val_loss: 0.4028 - val_acc: 0.8436\n",
      "Epoch 132/500\n",
      "12096/12096 [==============================] - 1s 121us/step - loss: 0.4443 - acc: 0.8333 - val_loss: 0.4032 - val_acc: 0.8479\n",
      "Epoch 133/500\n",
      "12096/12096 [==============================] - 2s 125us/step - loss: 0.4441 - acc: 0.8307 - val_loss: 0.4104 - val_acc: 0.8433\n",
      "Epoch 134/500\n",
      "12096/12096 [==============================] - 2s 125us/step - loss: 0.4388 - acc: 0.8347 - val_loss: 0.4088 - val_acc: 0.8433\n",
      "Epoch 135/500\n",
      "12096/12096 [==============================] - 1s 123us/step - loss: 0.4387 - acc: 0.8314 - val_loss: 0.4046 - val_acc: 0.8479\n",
      "Epoch 136/500\n",
      "12096/12096 [==============================] - 1s 122us/step - loss: 0.4457 - acc: 0.8277 - val_loss: 0.3985 - val_acc: 0.8485\n",
      "Epoch 137/500\n",
      "12096/12096 [==============================] - 1s 123us/step - loss: 0.4382 - acc: 0.8337 - val_loss: 0.4000 - val_acc: 0.8482\n",
      "Epoch 138/500\n",
      "12096/12096 [==============================] - 2s 124us/step - loss: 0.4412 - acc: 0.8313 - val_loss: 0.4071 - val_acc: 0.8413\n",
      "Epoch 139/500\n",
      "12096/12096 [==============================] - 1s 123us/step - loss: 0.4314 - acc: 0.8332 - val_loss: 0.4001 - val_acc: 0.8442\n",
      "Epoch 140/500\n",
      "12096/12096 [==============================] - 1s 124us/step - loss: 0.4299 - acc: 0.8371 - val_loss: 0.4001 - val_acc: 0.8469\n",
      "Epoch 141/500\n",
      "12096/12096 [==============================] - 1s 121us/step - loss: 0.4293 - acc: 0.8369 - val_loss: 0.3999 - val_acc: 0.8476\n",
      "Epoch 142/500\n",
      "12096/12096 [==============================] - 1s 122us/step - loss: 0.4304 - acc: 0.8333 - val_loss: 0.4040 - val_acc: 0.8492\n",
      "Epoch 143/500\n",
      "12096/12096 [==============================] - 1s 124us/step - loss: 0.4283 - acc: 0.8362 - val_loss: 0.4024 - val_acc: 0.8452\n",
      "Epoch 144/500\n",
      "12096/12096 [==============================] - 1s 121us/step - loss: 0.4293 - acc: 0.8376 - val_loss: 0.4101 - val_acc: 0.8452\n",
      "Epoch 145/500\n",
      "12096/12096 [==============================] - 2s 127us/step - loss: 0.4268 - acc: 0.8356 - val_loss: 0.4003 - val_acc: 0.8495\n",
      "Epoch 146/500\n",
      "12096/12096 [==============================] - 2s 125us/step - loss: 0.4350 - acc: 0.8375 - val_loss: 0.4027 - val_acc: 0.8472\n",
      "Epoch 147/500\n",
      "12096/12096 [==============================] - 2s 125us/step - loss: 0.4183 - acc: 0.8380 - val_loss: 0.3980 - val_acc: 0.8515\n",
      "Epoch 148/500\n",
      "12096/12096 [==============================] - 1s 122us/step - loss: 0.4271 - acc: 0.8390 - val_loss: 0.4044 - val_acc: 0.8479\n",
      "Epoch 149/500\n",
      "12096/12096 [==============================] - 2s 128us/step - loss: 0.4262 - acc: 0.8368 - val_loss: 0.3980 - val_acc: 0.8522\n",
      "Epoch 150/500\n",
      "12096/12096 [==============================] - 1s 121us/step - loss: 0.4282 - acc: 0.8378 - val_loss: 0.3989 - val_acc: 0.8492\n",
      "Epoch 151/500\n",
      "12096/12096 [==============================] - 1s 122us/step - loss: 0.4116 - acc: 0.8413 - val_loss: 0.4023 - val_acc: 0.8522\n",
      "Epoch 152/500\n",
      "12096/12096 [==============================] - 2s 128us/step - loss: 0.4200 - acc: 0.8380 - val_loss: 0.4018 - val_acc: 0.8542\n",
      "Epoch 153/500\n",
      "12096/12096 [==============================] - 1s 122us/step - loss: 0.4238 - acc: 0.8385 - val_loss: 0.3982 - val_acc: 0.8492\n",
      "Epoch 154/500\n",
      "12096/12096 [==============================] - 2s 128us/step - loss: 0.4177 - acc: 0.8390 - val_loss: 0.3978 - val_acc: 0.8548\n",
      "Epoch 155/500\n",
      "12096/12096 [==============================] - 1s 121us/step - loss: 0.4162 - acc: 0.8431 - val_loss: 0.3922 - val_acc: 0.8515\n",
      "Epoch 156/500\n",
      "12096/12096 [==============================] - 2s 125us/step - loss: 0.4124 - acc: 0.8428 - val_loss: 0.4051 - val_acc: 0.8502\n",
      "Epoch 157/500\n",
      "12096/12096 [==============================] - 1s 120us/step - loss: 0.4343 - acc: 0.8385 - val_loss: 0.3942 - val_acc: 0.8542\n",
      "Epoch 158/500\n",
      "12096/12096 [==============================] - 2s 125us/step - loss: 0.4178 - acc: 0.8397 - val_loss: 0.3949 - val_acc: 0.8456\n",
      "Epoch 159/500\n",
      "12096/12096 [==============================] - 1s 121us/step - loss: 0.4044 - acc: 0.8466 - val_loss: 0.3971 - val_acc: 0.8502\n",
      "Epoch 160/500\n",
      "12096/12096 [==============================] - 2s 128us/step - loss: 0.4048 - acc: 0.8456 - val_loss: 0.3909 - val_acc: 0.8585\n",
      "Epoch 161/500\n",
      "12096/12096 [==============================] - 2s 124us/step - loss: 0.4070 - acc: 0.8443 - val_loss: 0.3949 - val_acc: 0.8495\n",
      "Epoch 162/500\n",
      "12096/12096 [==============================] - 1s 122us/step - loss: 0.4196 - acc: 0.8384 - val_loss: 0.3953 - val_acc: 0.8545\n",
      "Epoch 163/500\n",
      "12096/12096 [==============================] - 2s 125us/step - loss: 0.4050 - acc: 0.8427 - val_loss: 0.3960 - val_acc: 0.8575\n",
      "Epoch 164/500\n",
      "12096/12096 [==============================] - 1s 122us/step - loss: 0.4127 - acc: 0.8461 - val_loss: 0.3968 - val_acc: 0.8578\n",
      "Epoch 165/500\n",
      "12096/12096 [==============================] - 1s 121us/step - loss: 0.4058 - acc: 0.8437 - val_loss: 0.4023 - val_acc: 0.8552\n",
      "Epoch 166/500\n",
      "12096/12096 [==============================] - 1s 123us/step - loss: 0.4052 - acc: 0.8483 - val_loss: 0.3985 - val_acc: 0.8565\n",
      "Epoch 167/500\n",
      "12096/12096 [==============================] - 2s 125us/step - loss: 0.3931 - acc: 0.8504 - val_loss: 0.3939 - val_acc: 0.8522\n",
      "Epoch 168/500\n",
      "12096/12096 [==============================] - 2s 126us/step - loss: 0.4065 - acc: 0.8457 - val_loss: 0.3895 - val_acc: 0.8588\n",
      "Epoch 169/500\n",
      "12096/12096 [==============================] - 2s 124us/step - loss: 0.4130 - acc: 0.8427 - val_loss: 0.3859 - val_acc: 0.8581\n",
      "Epoch 170/500\n",
      "12096/12096 [==============================] - 2s 124us/step - loss: 0.4023 - acc: 0.8436 - val_loss: 0.3889 - val_acc: 0.8568\n",
      "Epoch 171/500\n",
      "12096/12096 [==============================] - 1s 122us/step - loss: 0.4075 - acc: 0.8442 - val_loss: 0.3935 - val_acc: 0.8542\n",
      "Epoch 172/500\n",
      "12096/12096 [==============================] - 2s 125us/step - loss: 0.4063 - acc: 0.8442 - val_loss: 0.3933 - val_acc: 0.8548\n",
      "Epoch 173/500\n",
      "12096/12096 [==============================] - 1s 123us/step - loss: 0.3951 - acc: 0.8480 - val_loss: 0.3996 - val_acc: 0.8555\n",
      "Epoch 174/500\n",
      "12096/12096 [==============================] - 2s 125us/step - loss: 0.4015 - acc: 0.8476 - val_loss: 0.3957 - val_acc: 0.8598\n",
      "Epoch 175/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12096/12096 [==============================] - 1s 122us/step - loss: 0.3993 - acc: 0.8444 - val_loss: 0.3997 - val_acc: 0.8562\n",
      "Epoch 176/500\n",
      "12096/12096 [==============================] - 1s 122us/step - loss: 0.3985 - acc: 0.8475 - val_loss: 0.3920 - val_acc: 0.8525\n",
      "Epoch 177/500\n",
      "12096/12096 [==============================] - 1s 123us/step - loss: 0.4064 - acc: 0.8452 - val_loss: 0.3924 - val_acc: 0.8505\n",
      "Epoch 178/500\n",
      "12096/12096 [==============================] - 1s 121us/step - loss: 0.3992 - acc: 0.8494 - val_loss: 0.3911 - val_acc: 0.8548\n",
      "Epoch 179/500\n",
      "12096/12096 [==============================] - 1s 122us/step - loss: 0.3957 - acc: 0.8485 - val_loss: 0.3964 - val_acc: 0.8595\n",
      "Epoch 180/500\n",
      "12096/12096 [==============================] - 1s 123us/step - loss: 0.3944 - acc: 0.8523 - val_loss: 0.3916 - val_acc: 0.8598\n",
      "Epoch 181/500\n",
      "12096/12096 [==============================] - 1s 123us/step - loss: 0.3979 - acc: 0.8500 - val_loss: 0.3971 - val_acc: 0.8552\n",
      "Epoch 182/500\n",
      "12096/12096 [==============================] - 1s 122us/step - loss: 0.3939 - acc: 0.8486 - val_loss: 0.3904 - val_acc: 0.8588\n",
      "Epoch 183/500\n",
      "12096/12096 [==============================] - 1s 123us/step - loss: 0.3973 - acc: 0.8487 - val_loss: 0.3882 - val_acc: 0.8571\n",
      "Epoch 184/500\n",
      "12096/12096 [==============================] - 1s 122us/step - loss: 0.3977 - acc: 0.8485 - val_loss: 0.3924 - val_acc: 0.8581\n",
      "Epoch 185/500\n",
      "12096/12096 [==============================] - 1s 121us/step - loss: 0.3931 - acc: 0.8519 - val_loss: 0.3897 - val_acc: 0.8555\n",
      "Epoch 186/500\n",
      "12096/12096 [==============================] - 1s 122us/step - loss: 0.3938 - acc: 0.8547 - val_loss: 0.3903 - val_acc: 0.8545\n",
      "Epoch 187/500\n",
      "12096/12096 [==============================] - 1s 121us/step - loss: 0.3898 - acc: 0.8533 - val_loss: 0.3865 - val_acc: 0.8591\n",
      "Epoch 188/500\n",
      "12096/12096 [==============================] - 1s 123us/step - loss: 0.3936 - acc: 0.8507 - val_loss: 0.3843 - val_acc: 0.8588\n",
      "Epoch 189/500\n",
      "12096/12096 [==============================] - 1s 123us/step - loss: 0.3924 - acc: 0.8513 - val_loss: 0.3838 - val_acc: 0.8552\n",
      "Epoch 190/500\n",
      "12096/12096 [==============================] - 1s 122us/step - loss: 0.3835 - acc: 0.8534 - val_loss: 0.3909 - val_acc: 0.8558\n",
      "Epoch 191/500\n",
      "12096/12096 [==============================] - 1s 122us/step - loss: 0.3915 - acc: 0.8496 - val_loss: 0.3844 - val_acc: 0.8562\n",
      "Epoch 192/500\n",
      "12096/12096 [==============================] - 1s 123us/step - loss: 0.3987 - acc: 0.8484 - val_loss: 0.3889 - val_acc: 0.8568\n",
      "Epoch 193/500\n",
      "12096/12096 [==============================] - 1s 123us/step - loss: 0.3993 - acc: 0.8500 - val_loss: 0.3908 - val_acc: 0.8558\n",
      "Epoch 194/500\n",
      "12096/12096 [==============================] - 1s 122us/step - loss: 0.4083 - acc: 0.8452 - val_loss: 0.3879 - val_acc: 0.8578\n",
      "Epoch 195/500\n",
      "12096/12096 [==============================] - 1s 123us/step - loss: 0.3845 - acc: 0.8527 - val_loss: 0.3922 - val_acc: 0.8565\n",
      "Epoch 196/500\n",
      "12096/12096 [==============================] - 1s 121us/step - loss: 0.3979 - acc: 0.8509 - val_loss: 0.3821 - val_acc: 0.8598\n",
      "Epoch 197/500\n",
      "12096/12096 [==============================] - 1s 120us/step - loss: 0.3861 - acc: 0.8535 - val_loss: 0.3887 - val_acc: 0.8552\n",
      "Epoch 198/500\n",
      "12096/12096 [==============================] - 1s 123us/step - loss: 0.3968 - acc: 0.8500 - val_loss: 0.3946 - val_acc: 0.8575\n",
      "Epoch 199/500\n",
      "12096/12096 [==============================] - 1s 123us/step - loss: 0.3861 - acc: 0.8536 - val_loss: 0.3971 - val_acc: 0.8538\n",
      "Epoch 200/500\n",
      "12096/12096 [==============================] - 1s 122us/step - loss: 0.3794 - acc: 0.8574 - val_loss: 0.3881 - val_acc: 0.8591\n",
      "Epoch 201/500\n",
      "12096/12096 [==============================] - 1s 122us/step - loss: 0.3840 - acc: 0.8562 - val_loss: 0.3904 - val_acc: 0.8545\n",
      "Epoch 202/500\n",
      "12096/12096 [==============================] - 1s 121us/step - loss: 0.3940 - acc: 0.8538 - val_loss: 0.3887 - val_acc: 0.8571\n",
      "Epoch 203/500\n",
      "12096/12096 [==============================] - 1s 123us/step - loss: 0.3834 - acc: 0.8594 - val_loss: 0.3877 - val_acc: 0.8565\n",
      "Epoch 204/500\n",
      "12096/12096 [==============================] - 2s 127us/step - loss: 0.3813 - acc: 0.8551 - val_loss: 0.3820 - val_acc: 0.8618\n",
      "Epoch 205/500\n",
      "12096/12096 [==============================] - 1s 123us/step - loss: 0.3766 - acc: 0.8594 - val_loss: 0.3890 - val_acc: 0.8568\n",
      "Epoch 206/500\n",
      "12096/12096 [==============================] - 2s 125us/step - loss: 0.3875 - acc: 0.8542 - val_loss: 0.3866 - val_acc: 0.8598\n",
      "Epoch 207/500\n",
      "12096/12096 [==============================] - 1s 122us/step - loss: 0.3795 - acc: 0.8578 - val_loss: 0.3961 - val_acc: 0.8595\n",
      "Epoch 208/500\n",
      "12096/12096 [==============================] - 2s 125us/step - loss: 0.3869 - acc: 0.8556 - val_loss: 0.3869 - val_acc: 0.8555\n",
      "Epoch 209/500\n",
      "12096/12096 [==============================] - 1s 122us/step - loss: 0.3823 - acc: 0.8556 - val_loss: 0.3914 - val_acc: 0.8558\n",
      "Epoch 210/500\n",
      "12096/12096 [==============================] - 1s 122us/step - loss: 0.3871 - acc: 0.8523 - val_loss: 0.3780 - val_acc: 0.8552\n",
      "Epoch 211/500\n",
      "12096/12096 [==============================] - 2s 125us/step - loss: 0.3802 - acc: 0.8575 - val_loss: 0.3823 - val_acc: 0.8595\n",
      "Epoch 212/500\n",
      "12096/12096 [==============================] - 2s 128us/step - loss: 0.3851 - acc: 0.8519 - val_loss: 0.3839 - val_acc: 0.8621\n",
      "Epoch 213/500\n",
      "12096/12096 [==============================] - 2s 127us/step - loss: 0.3786 - acc: 0.8557 - val_loss: 0.3848 - val_acc: 0.8634\n",
      "Epoch 214/500\n",
      "12096/12096 [==============================] - 1s 122us/step - loss: 0.3703 - acc: 0.8628 - val_loss: 0.3854 - val_acc: 0.8604\n",
      "Epoch 215/500\n",
      "12096/12096 [==============================] - 1s 122us/step - loss: 0.3782 - acc: 0.8552 - val_loss: 0.3903 - val_acc: 0.8624\n",
      "Epoch 216/500\n",
      "12096/12096 [==============================] - 1s 122us/step - loss: 0.3727 - acc: 0.8609 - val_loss: 0.3876 - val_acc: 0.8634\n",
      "Epoch 217/500\n",
      "12096/12096 [==============================] - 2s 124us/step - loss: 0.3864 - acc: 0.8529 - val_loss: 0.3882 - val_acc: 0.8634\n",
      "Epoch 218/500\n",
      "12096/12096 [==============================] - 1s 123us/step - loss: 0.3848 - acc: 0.8542 - val_loss: 0.3757 - val_acc: 0.8608\n",
      "Epoch 219/500\n",
      "12096/12096 [==============================] - 1s 122us/step - loss: 0.3803 - acc: 0.8574 - val_loss: 0.3815 - val_acc: 0.8614\n",
      "Epoch 220/500\n",
      "12096/12096 [==============================] - 1s 122us/step - loss: 0.3765 - acc: 0.8588 - val_loss: 0.3816 - val_acc: 0.8621\n",
      "Epoch 221/500\n",
      "12096/12096 [==============================] - 2s 128us/step - loss: 0.3633 - acc: 0.8607 - val_loss: 0.3801 - val_acc: 0.8667\n",
      "Epoch 222/500\n",
      "12096/12096 [==============================] - 1s 124us/step - loss: 0.3593 - acc: 0.8625 - val_loss: 0.3925 - val_acc: 0.8604\n",
      "Epoch 223/500\n",
      "12096/12096 [==============================] - 1s 122us/step - loss: 0.3746 - acc: 0.8572 - val_loss: 0.3900 - val_acc: 0.8628\n",
      "Epoch 224/500\n",
      "12096/12096 [==============================] - 2s 124us/step - loss: 0.3754 - acc: 0.8595 - val_loss: 0.3928 - val_acc: 0.8591\n",
      "Epoch 225/500\n",
      "12096/12096 [==============================] - 1s 123us/step - loss: 0.3720 - acc: 0.8654 - val_loss: 0.3784 - val_acc: 0.8614\n",
      "Epoch 226/500\n",
      "12096/12096 [==============================] - 1s 123us/step - loss: 0.3743 - acc: 0.8604 - val_loss: 0.3899 - val_acc: 0.8585\n",
      "Epoch 227/500\n",
      "12096/12096 [==============================] - 1s 121us/step - loss: 0.3690 - acc: 0.8639 - val_loss: 0.3904 - val_acc: 0.8631\n",
      "Epoch 228/500\n",
      "12096/12096 [==============================] - 2s 124us/step - loss: 0.3660 - acc: 0.8641 - val_loss: 0.3834 - val_acc: 0.8644\n",
      "Epoch 229/500\n",
      "12096/12096 [==============================] - 2s 127us/step - loss: 0.3768 - acc: 0.8612 - val_loss: 0.3907 - val_acc: 0.8604\n",
      "Epoch 230/500\n",
      "12096/12096 [==============================] - 1s 123us/step - loss: 0.3761 - acc: 0.8586 - val_loss: 0.3878 - val_acc: 0.8651\n",
      "Epoch 231/500\n",
      "12096/12096 [==============================] - 2s 126us/step - loss: 0.3625 - acc: 0.8669 - val_loss: 0.3866 - val_acc: 0.8604\n",
      "Epoch 232/500\n",
      "12096/12096 [==============================] - 2s 125us/step - loss: 0.3601 - acc: 0.8624 - val_loss: 0.3925 - val_acc: 0.8608\n",
      "Epoch 233/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12096/12096 [==============================] - 1s 123us/step - loss: 0.3598 - acc: 0.8642 - val_loss: 0.3785 - val_acc: 0.8621\n",
      "Epoch 234/500\n",
      "12096/12096 [==============================] - 1s 123us/step - loss: 0.3703 - acc: 0.8617 - val_loss: 0.3852 - val_acc: 0.8631\n",
      "Epoch 235/500\n",
      "12096/12096 [==============================] - 2s 125us/step - loss: 0.3592 - acc: 0.8651 - val_loss: 0.3837 - val_acc: 0.8641\n",
      "Epoch 236/500\n",
      "12096/12096 [==============================] - 1s 122us/step - loss: 0.3679 - acc: 0.8570 - val_loss: 0.3853 - val_acc: 0.8621\n",
      "Epoch 237/500\n",
      "12096/12096 [==============================] - 1s 123us/step - loss: 0.3649 - acc: 0.8600 - val_loss: 0.3852 - val_acc: 0.8611\n",
      "Epoch 238/500\n",
      "12096/12096 [==============================] - 1s 122us/step - loss: 0.3666 - acc: 0.8631 - val_loss: 0.3849 - val_acc: 0.8601\n",
      "Epoch 239/500\n",
      "12096/12096 [==============================] - 2s 125us/step - loss: 0.3601 - acc: 0.8637 - val_loss: 0.3791 - val_acc: 0.8631\n",
      "Epoch 240/500\n",
      "12096/12096 [==============================] - 2s 125us/step - loss: 0.3639 - acc: 0.8619 - val_loss: 0.3852 - val_acc: 0.8614\n",
      "Epoch 241/500\n",
      "12096/12096 [==============================] - 1s 118us/step - loss: 0.3715 - acc: 0.8606 - val_loss: 0.3826 - val_acc: 0.8614\n",
      "Epoch 242/500\n",
      "12096/12096 [==============================] - 1s 121us/step - loss: 0.3595 - acc: 0.8625 - val_loss: 0.3859 - val_acc: 0.8548\n",
      "Epoch 243/500\n",
      "12096/12096 [==============================] - 2s 125us/step - loss: 0.3628 - acc: 0.8618 - val_loss: 0.3817 - val_acc: 0.8618\n",
      "Epoch 244/500\n",
      "12096/12096 [==============================] - 1s 123us/step - loss: 0.3552 - acc: 0.8657 - val_loss: 0.3882 - val_acc: 0.8565\n",
      "Epoch 245/500\n",
      "12096/12096 [==============================] - 1s 121us/step - loss: 0.3602 - acc: 0.8627 - val_loss: 0.3792 - val_acc: 0.8614\n",
      "Epoch 246/500\n",
      "12096/12096 [==============================] - 1s 120us/step - loss: 0.3635 - acc: 0.8624 - val_loss: 0.3770 - val_acc: 0.8638\n",
      "Epoch 247/500\n",
      "12096/12096 [==============================] - 1s 122us/step - loss: 0.3606 - acc: 0.8664 - val_loss: 0.3803 - val_acc: 0.8641\n",
      "Epoch 248/500\n",
      "12096/12096 [==============================] - 1s 119us/step - loss: 0.3641 - acc: 0.8604 - val_loss: 0.3897 - val_acc: 0.8601\n",
      "Epoch 249/500\n",
      "12096/12096 [==============================] - 1s 122us/step - loss: 0.3546 - acc: 0.8671 - val_loss: 0.3867 - val_acc: 0.8624\n",
      "Epoch 250/500\n",
      "12096/12096 [==============================] - 1s 121us/step - loss: 0.3654 - acc: 0.8624 - val_loss: 0.3863 - val_acc: 0.8618\n",
      "Epoch 251/500\n",
      "12096/12096 [==============================] - 1s 123us/step - loss: 0.3514 - acc: 0.8638 - val_loss: 0.3851 - val_acc: 0.8638\n",
      "Epoch 252/500\n",
      "12096/12096 [==============================] - 1s 122us/step - loss: 0.3595 - acc: 0.8650 - val_loss: 0.3866 - val_acc: 0.8604\n",
      "Epoch 253/500\n",
      "12096/12096 [==============================] - 1s 121us/step - loss: 0.3515 - acc: 0.8671 - val_loss: 0.3849 - val_acc: 0.8571\n",
      "Epoch 254/500\n",
      "12096/12096 [==============================] - 1s 122us/step - loss: 0.3676 - acc: 0.8624 - val_loss: 0.3764 - val_acc: 0.8634\n",
      "Epoch 255/500\n",
      "12096/12096 [==============================] - 1s 122us/step - loss: 0.3570 - acc: 0.8652 - val_loss: 0.3803 - val_acc: 0.8604\n",
      "Epoch 256/500\n",
      "12096/12096 [==============================] - 1s 122us/step - loss: 0.3500 - acc: 0.8691 - val_loss: 0.3785 - val_acc: 0.8661\n",
      "Epoch 257/500\n",
      "12096/12096 [==============================] - 1s 123us/step - loss: 0.3594 - acc: 0.8670 - val_loss: 0.3806 - val_acc: 0.8661\n",
      "Epoch 258/500\n",
      "12096/12096 [==============================] - 1s 124us/step - loss: 0.3605 - acc: 0.8641 - val_loss: 0.3804 - val_acc: 0.8641\n",
      "Epoch 259/500\n",
      "12096/12096 [==============================] - 1s 121us/step - loss: 0.3624 - acc: 0.8612 - val_loss: 0.3772 - val_acc: 0.8647\n",
      "Epoch 260/500\n",
      "12096/12096 [==============================] - 1s 123us/step - loss: 0.3570 - acc: 0.8667 - val_loss: 0.3817 - val_acc: 0.8661\n",
      "Epoch 261/500\n",
      "12096/12096 [==============================] - 1s 122us/step - loss: 0.3565 - acc: 0.8627 - val_loss: 0.3860 - val_acc: 0.8654\n",
      "Epoch 262/500\n",
      "12096/12096 [==============================] - 1s 123us/step - loss: 0.3554 - acc: 0.8662 - val_loss: 0.3784 - val_acc: 0.8608\n",
      "Epoch 263/500\n",
      "12096/12096 [==============================] - 1s 122us/step - loss: 0.3569 - acc: 0.8633 - val_loss: 0.3898 - val_acc: 0.8624\n",
      "Epoch 264/500\n",
      "12096/12096 [==============================] - 1s 122us/step - loss: 0.3499 - acc: 0.8688 - val_loss: 0.3877 - val_acc: 0.8644\n",
      "Epoch 265/500\n",
      "12096/12096 [==============================] - 1s 123us/step - loss: 0.3606 - acc: 0.8669 - val_loss: 0.3815 - val_acc: 0.8644\n",
      "Epoch 266/500\n",
      "12096/12096 [==============================] - 2s 126us/step - loss: 0.3467 - acc: 0.8686 - val_loss: 0.3836 - val_acc: 0.8671\n",
      "Epoch 267/500\n",
      "12096/12096 [==============================] - 2s 125us/step - loss: 0.3539 - acc: 0.8695 - val_loss: 0.3858 - val_acc: 0.8618\n",
      "Epoch 268/500\n",
      "12096/12096 [==============================] - 2s 124us/step - loss: 0.3523 - acc: 0.8674 - val_loss: 0.3911 - val_acc: 0.8628\n",
      "Epoch 269/500\n",
      "12096/12096 [==============================] - 1s 123us/step - loss: 0.3561 - acc: 0.8676 - val_loss: 0.3898 - val_acc: 0.8661\n",
      "Epoch 270/500\n",
      "12096/12096 [==============================] - 1s 124us/step - loss: 0.3621 - acc: 0.8657 - val_loss: 0.3895 - val_acc: 0.8608\n",
      "Epoch 271/500\n",
      "12096/12096 [==============================] - 2s 125us/step - loss: 0.3467 - acc: 0.8719 - val_loss: 0.3889 - val_acc: 0.8638\n",
      "Epoch 272/500\n",
      "12096/12096 [==============================] - 1s 123us/step - loss: 0.3559 - acc: 0.8670 - val_loss: 0.3966 - val_acc: 0.8618\n",
      "Epoch 273/500\n",
      "12096/12096 [==============================] - 1s 122us/step - loss: 0.3487 - acc: 0.8698 - val_loss: 0.3857 - val_acc: 0.8654\n",
      "Epoch 274/500\n",
      "12096/12096 [==============================] - 1s 123us/step - loss: 0.3415 - acc: 0.8729 - val_loss: 0.3883 - val_acc: 0.8611\n",
      "Epoch 275/500\n",
      "12096/12096 [==============================] - 1s 122us/step - loss: 0.3592 - acc: 0.8649 - val_loss: 0.3887 - val_acc: 0.8618\n",
      "Epoch 276/500\n",
      "12096/12096 [==============================] - 1s 122us/step - loss: 0.3502 - acc: 0.8715 - val_loss: 0.3794 - val_acc: 0.8638\n",
      "Epoch 277/500\n",
      "12096/12096 [==============================] - 1s 122us/step - loss: 0.3362 - acc: 0.8735 - val_loss: 0.3896 - val_acc: 0.8624\n",
      "Epoch 278/500\n",
      "12096/12096 [==============================] - 1s 122us/step - loss: 0.3434 - acc: 0.8719 - val_loss: 0.3831 - val_acc: 0.8664\n",
      "Epoch 279/500\n",
      "12096/12096 [==============================] - 1s 123us/step - loss: 0.3357 - acc: 0.8678 - val_loss: 0.3939 - val_acc: 0.8618\n",
      "Epoch 280/500\n",
      "12096/12096 [==============================] - 1s 123us/step - loss: 0.3412 - acc: 0.8744 - val_loss: 0.3864 - val_acc: 0.8614\n",
      "Epoch 281/500\n",
      "12096/12096 [==============================] - 2s 129us/step - loss: 0.3428 - acc: 0.8734 - val_loss: 0.3794 - val_acc: 0.8704\n",
      "Epoch 282/500\n",
      "12096/12096 [==============================] - 1s 122us/step - loss: 0.3434 - acc: 0.8658 - val_loss: 0.3792 - val_acc: 0.8661\n",
      "Epoch 283/500\n",
      "12096/12096 [==============================] - 1s 122us/step - loss: 0.3443 - acc: 0.8749 - val_loss: 0.3784 - val_acc: 0.8684\n",
      "Epoch 284/500\n",
      "12096/12096 [==============================] - 1s 123us/step - loss: 0.3483 - acc: 0.8731 - val_loss: 0.3709 - val_acc: 0.8690\n",
      "Epoch 285/500\n",
      "12096/12096 [==============================] - 2s 124us/step - loss: 0.3408 - acc: 0.8746 - val_loss: 0.3782 - val_acc: 0.8704\n",
      "Epoch 286/500\n",
      "12096/12096 [==============================] - 1s 123us/step - loss: 0.3467 - acc: 0.8681 - val_loss: 0.3830 - val_acc: 0.8677\n",
      "Epoch 287/500\n",
      "12096/12096 [==============================] - 2s 125us/step - loss: 0.3449 - acc: 0.8716 - val_loss: 0.3810 - val_acc: 0.8677\n",
      "Epoch 288/500\n",
      "12096/12096 [==============================] - 1s 122us/step - loss: 0.3423 - acc: 0.8709 - val_loss: 0.3792 - val_acc: 0.8700\n",
      "Epoch 289/500\n",
      "12096/12096 [==============================] - 1s 124us/step - loss: 0.3363 - acc: 0.8785 - val_loss: 0.3835 - val_acc: 0.8661\n",
      "Epoch 290/500\n",
      "12096/12096 [==============================] - 1s 123us/step - loss: 0.3363 - acc: 0.8740 - val_loss: 0.3804 - val_acc: 0.8641\n",
      "Epoch 291/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12096/12096 [==============================] - 1s 120us/step - loss: 0.3415 - acc: 0.8750 - val_loss: 0.3798 - val_acc: 0.8700\n",
      "Epoch 292/500\n",
      "12096/12096 [==============================] - 1s 124us/step - loss: 0.3386 - acc: 0.8735 - val_loss: 0.3720 - val_acc: 0.8671\n",
      "Epoch 293/500\n",
      "12096/12096 [==============================] - 1s 123us/step - loss: 0.3445 - acc: 0.8678 - val_loss: 0.3846 - val_acc: 0.8647\n",
      "Epoch 294/500\n",
      "12096/12096 [==============================] - 1s 122us/step - loss: 0.3401 - acc: 0.8772 - val_loss: 0.3795 - val_acc: 0.8697\n",
      "Epoch 295/500\n",
      "12096/12096 [==============================] - 1s 123us/step - loss: 0.3333 - acc: 0.8724 - val_loss: 0.3855 - val_acc: 0.8624\n",
      "Epoch 296/500\n",
      "12096/12096 [==============================] - 1s 120us/step - loss: 0.3350 - acc: 0.8764 - val_loss: 0.3826 - val_acc: 0.8651\n",
      "Epoch 297/500\n",
      "12096/12096 [==============================] - 1s 124us/step - loss: 0.3338 - acc: 0.8761 - val_loss: 0.3796 - val_acc: 0.8664\n",
      "Epoch 298/500\n",
      "12096/12096 [==============================] - 1s 123us/step - loss: 0.3378 - acc: 0.8734 - val_loss: 0.3892 - val_acc: 0.8667\n",
      "Epoch 299/500\n",
      "12096/12096 [==============================] - 1s 122us/step - loss: 0.3381 - acc: 0.8742 - val_loss: 0.3854 - val_acc: 0.8664\n",
      "Epoch 300/500\n",
      "12096/12096 [==============================] - 1s 124us/step - loss: 0.3438 - acc: 0.8758 - val_loss: 0.3808 - val_acc: 0.8661\n",
      "Epoch 301/500\n",
      "12096/12096 [==============================] - 1s 122us/step - loss: 0.3371 - acc: 0.8764 - val_loss: 0.3861 - val_acc: 0.8647\n",
      "Epoch 302/500\n",
      "12096/12096 [==============================] - 1s 121us/step - loss: 0.3313 - acc: 0.8750 - val_loss: 0.3815 - val_acc: 0.8638\n",
      "Epoch 303/500\n",
      "12096/12096 [==============================] - 1s 123us/step - loss: 0.3340 - acc: 0.8764 - val_loss: 0.3870 - val_acc: 0.8608\n",
      "Epoch 304/500\n",
      "12096/12096 [==============================] - 1s 121us/step - loss: 0.3432 - acc: 0.8733 - val_loss: 0.3885 - val_acc: 0.8671\n",
      "Epoch 305/500\n",
      "12096/12096 [==============================] - 1s 121us/step - loss: 0.3348 - acc: 0.8715 - val_loss: 0.3902 - val_acc: 0.8654\n",
      "Epoch 306/500\n",
      "12096/12096 [==============================] - 1s 124us/step - loss: 0.3370 - acc: 0.8752 - val_loss: 0.3776 - val_acc: 0.8661\n",
      "Epoch 307/500\n",
      "12096/12096 [==============================] - 1s 120us/step - loss: 0.3287 - acc: 0.8762 - val_loss: 0.3865 - val_acc: 0.8671\n",
      "Epoch 308/500\n",
      "12096/12096 [==============================] - 2s 128us/step - loss: 0.3448 - acc: 0.8705 - val_loss: 0.3812 - val_acc: 0.8707\n",
      "Epoch 309/500\n",
      "12096/12096 [==============================] - 1s 122us/step - loss: 0.3409 - acc: 0.8726 - val_loss: 0.3771 - val_acc: 0.8697\n",
      "Epoch 310/500\n",
      "12096/12096 [==============================] - 2s 125us/step - loss: 0.3435 - acc: 0.8714 - val_loss: 0.3751 - val_acc: 0.8690\n",
      "Epoch 311/500\n",
      "12096/12096 [==============================] - 1s 122us/step - loss: 0.3285 - acc: 0.8767 - val_loss: 0.3838 - val_acc: 0.8700\n",
      "Epoch 312/500\n",
      "12096/12096 [==============================] - 2s 124us/step - loss: 0.3338 - acc: 0.8751 - val_loss: 0.3816 - val_acc: 0.8677\n",
      "Epoch 313/500\n",
      "12096/12096 [==============================] - 1s 122us/step - loss: 0.3335 - acc: 0.8765 - val_loss: 0.3881 - val_acc: 0.8638\n",
      "Epoch 314/500\n",
      "12096/12096 [==============================] - 1s 123us/step - loss: 0.3180 - acc: 0.8815 - val_loss: 0.3948 - val_acc: 0.8664\n",
      "Epoch 315/500\n",
      "12096/12096 [==============================] - 1s 123us/step - loss: 0.3357 - acc: 0.8738 - val_loss: 0.3836 - val_acc: 0.8664\n",
      "Epoch 316/500\n",
      "12096/12096 [==============================] - 1s 122us/step - loss: 0.3324 - acc: 0.8765 - val_loss: 0.3915 - val_acc: 0.8654\n",
      "Epoch 317/500\n",
      "12096/12096 [==============================] - 1s 122us/step - loss: 0.3351 - acc: 0.8764 - val_loss: 0.3830 - val_acc: 0.8647\n",
      "Epoch 318/500\n",
      "12096/12096 [==============================] - 1s 124us/step - loss: 0.3316 - acc: 0.8768 - val_loss: 0.3855 - val_acc: 0.8651\n",
      "Epoch 319/500\n",
      "12096/12096 [==============================] - 1s 122us/step - loss: 0.3353 - acc: 0.8729 - val_loss: 0.3824 - val_acc: 0.8651\n",
      "Epoch 320/500\n",
      "12096/12096 [==============================] - 1s 123us/step - loss: 0.3207 - acc: 0.8800 - val_loss: 0.3898 - val_acc: 0.8644\n",
      "Epoch 321/500\n",
      "12096/12096 [==============================] - 1s 123us/step - loss: 0.3283 - acc: 0.8771 - val_loss: 0.3831 - val_acc: 0.8651\n",
      "Epoch 322/500\n",
      "12096/12096 [==============================] - 2s 125us/step - loss: 0.3187 - acc: 0.8805 - val_loss: 0.3826 - val_acc: 0.8628\n",
      "Epoch 323/500\n",
      "12096/12096 [==============================] - 1s 124us/step - loss: 0.3350 - acc: 0.8745 - val_loss: 0.3837 - val_acc: 0.8671\n",
      "Epoch 324/500\n",
      "12096/12096 [==============================] - 1s 122us/step - loss: 0.3391 - acc: 0.8713 - val_loss: 0.3756 - val_acc: 0.8638\n",
      "Epoch 325/500\n",
      "12096/12096 [==============================] - 1s 124us/step - loss: 0.3261 - acc: 0.8798 - val_loss: 0.3739 - val_acc: 0.8667\n",
      "Epoch 326/500\n",
      "12096/12096 [==============================] - 1s 121us/step - loss: 0.3224 - acc: 0.8797 - val_loss: 0.3778 - val_acc: 0.8704\n",
      "Epoch 327/500\n",
      "12096/12096 [==============================] - 1s 123us/step - loss: 0.3210 - acc: 0.8798 - val_loss: 0.3842 - val_acc: 0.8667\n",
      "Epoch 328/500\n",
      "12096/12096 [==============================] - 2s 125us/step - loss: 0.3325 - acc: 0.8773 - val_loss: 0.3834 - val_acc: 0.8651\n",
      "Epoch 329/500\n",
      "12096/12096 [==============================] - 2s 129us/step - loss: 0.3207 - acc: 0.8785 - val_loss: 0.3789 - val_acc: 0.8710\n",
      "Epoch 330/500\n",
      "12096/12096 [==============================] - 1s 123us/step - loss: 0.3217 - acc: 0.8841 - val_loss: 0.3848 - val_acc: 0.8687\n",
      "Epoch 331/500\n",
      "12096/12096 [==============================] - 1s 121us/step - loss: 0.3325 - acc: 0.8781 - val_loss: 0.3770 - val_acc: 0.8681\n",
      "Epoch 332/500\n",
      "12096/12096 [==============================] - 1s 122us/step - loss: 0.3218 - acc: 0.8803 - val_loss: 0.3778 - val_acc: 0.8707\n",
      "Epoch 333/500\n",
      "12096/12096 [==============================] - 2s 125us/step - loss: 0.3167 - acc: 0.8825 - val_loss: 0.3866 - val_acc: 0.8684\n",
      "Epoch 334/500\n",
      "12096/12096 [==============================] - 2s 125us/step - loss: 0.3284 - acc: 0.8802 - val_loss: 0.3784 - val_acc: 0.8694\n",
      "Epoch 335/500\n",
      "12096/12096 [==============================] - 2s 125us/step - loss: 0.3206 - acc: 0.8805 - val_loss: 0.3794 - val_acc: 0.8694\n",
      "Epoch 336/500\n",
      "12096/12096 [==============================] - 1s 123us/step - loss: 0.3232 - acc: 0.8805 - val_loss: 0.3779 - val_acc: 0.8700\n",
      "Epoch 337/500\n",
      "12096/12096 [==============================] - 1s 122us/step - loss: 0.3260 - acc: 0.8801 - val_loss: 0.3801 - val_acc: 0.8681\n",
      "Epoch 338/500\n",
      "12096/12096 [==============================] - 2s 125us/step - loss: 0.3188 - acc: 0.8823 - val_loss: 0.3818 - val_acc: 0.8700\n",
      "Epoch 339/500\n",
      "12096/12096 [==============================] - 1s 123us/step - loss: 0.3222 - acc: 0.8807 - val_loss: 0.3875 - val_acc: 0.8694\n",
      "Epoch 340/500\n",
      "12096/12096 [==============================] - 2s 125us/step - loss: 0.3273 - acc: 0.8776 - val_loss: 0.3796 - val_acc: 0.8697\n",
      "Epoch 341/500\n",
      "12096/12096 [==============================] - 2s 125us/step - loss: 0.3273 - acc: 0.8785 - val_loss: 0.3808 - val_acc: 0.8700\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f0ad41e5160>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, CSVLogger\n",
    "\n",
    "name = 'mlp_02'\n",
    "\n",
    "df_train, df_test = get_train_test_data(add_soil_features=True)\n",
    "model2, X_scaled, one_hot_labels, scaler = prepare_data_model(df_train, df_test)\n",
    "\n",
    "# log history\n",
    "history_name = 'history/' + name + '.csv'\n",
    "historyLogger = CSVLogger(history_name, append=False)\n",
    "\n",
    "# safe model checkpoints\n",
    "checkpoint_name = 'checkpoints/' + name + '.hdf5'\n",
    "modelCheckPoint = ModelCheckpoint(checkpoint_name, monitor='val_acc', verbose=0, save_best_only=True)\n",
    "\n",
    "# stop training when validation loss is not decreasing\n",
    "earlyStopping = EarlyStopping(monitor='val_acc', min_delta=0.001, patience=60)\n",
    "\n",
    "# list with callbacks for fit_generator\n",
    "callbacks = [historyLogger, modelCheckPoint, earlyStopping]\n",
    "\n",
    "model2.fit(X_scaled, one_hot_labels, epochs=MAX_EPOCHS, batch_size=64, validation_split=0.2, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0ad4c44a90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "history = pd.read_csv(history_name)\n",
    "\n",
    "fig, ax = plt.subplots(1,1)\n",
    "ax.set_title('Accuracy for train and test set')\n",
    "ax.set_xlabel('Epochs')\n",
    "ax.set_ylabel('Accuracy')\n",
    "\n",
    "x_plt = np.arange(0, len(history))\n",
    "l1 = ax.plot(x_plt, history['acc'], label='Train')\n",
    "l2 = ax.plot(x_plt, history['val_acc'], label='Test')\n",
    "\n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "X_test = np.array(df_test.drop(['Id'], axis=1))\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "model2.load_weights(checkpoint_name)\n",
    "pred = model2.predict(X_test_scaled)\n",
    "# convert from range 0..6 back to 1..7\n",
    "y_pred = np.argmax(pred, axis=1) + 1\n",
    "\n",
    "df_test = df_test.iloc[:, df_test.columns == 'Id']\n",
    "df_test['Cover_Type'] = y_pred\n",
    "\n",
    "df_test.to_csv('result_nn.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
